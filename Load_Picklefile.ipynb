{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.1\n",
      "1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load all models from pickle files and see results.\n",
    "\n",
    "# Challenges - will not be able to see features with their weights for RandomForestClassifier or VoteClassifier\n",
    "# Will need to use LinearSVC to see the top words with their associated weights - use the \"coef_\" function\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from math import sqrt\n",
    "import joblib\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import datetime\n",
    "import html\n",
    "import os\n",
    "import timeit\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# sklearn=0.23.1, pandas=1.0.1\n",
    "print(sklearn.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Sources:\n",
      "Huffington:\n",
      "We have 4,355 records\n",
      "We have 4,314 records > 800 characters long\n",
      "4,304 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 4074 / 4304\n",
      "Entire dataset accuracy: 0.9568\n",
      "\n",
      "Slate:\n",
      "We have 1,566 records\n",
      "We have 1,498 records > 800 characters long\n",
      "1,156 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 869 / 1156\n",
      "Entire dataset accuracy: 0.7837\n",
      "\n",
      "Salon:\n",
      "We have 2,303 records\n",
      "We have 2,278 records > 800 characters long\n",
      "1,987 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 1644 / 1987\n",
      "Entire dataset accuracy: 0.8546\n",
      "\n",
      "TalkingPointsMemo:\n",
      "We have 2,888 records\n",
      "We have 2,820 records > 800 characters long\n",
      "1,896 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 1056 / 1896\n",
      "Entire dataset accuracy: 0.5955\n",
      "\n",
      "Alternet:\n",
      "We have 963 records\n",
      "We have 931 records > 800 characters long\n",
      "929 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 596 / 929\n",
      "Entire dataset accuracy: 0.6728\n",
      "\n",
      "Rawstory:\n",
      "We have 1,160 records\n",
      "We have 1,159 records > 800 characters long\n",
      "1,159 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 892 / 1159\n",
      "Entire dataset accuracy: 0.7955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a model and test on some entries - Try testing on an entire dataset of another source's articles!\n",
    "text_clf = joblib.load('h_and_n_trigram_sr_nol_nopr.pkl')\n",
    "length_req = 800  # Character length requirement for articles\n",
    "confidence_level = .53\n",
    "\n",
    "left_source = [\"Huffington\", \"Slate\", \"Salon\", \"TalkingPointsMemo\", \"Alternet\", \"Rawstory\"]\n",
    "\n",
    "print(\"Left Sources:\")\n",
    "for source in left_source:\n",
    "    test_data = pd.read_excel(\"csvs/\" + source + \".xlsx\", \n",
    "                  names=[\"date\", \"article\"])\n",
    "    \n",
    "    print(source + \":\")\n",
    "    print(\"We have {:,} records\".format(test_data.shape[0]))\n",
    "    \n",
    "    test_data['date'].fillna(\"\", inplace=True)\n",
    "    test_data['article'].fillna(\"\", inplace=True)\n",
    "\n",
    "    for x in range(test_data.shape[0]):\n",
    "        if len(test_data['article'][x]) < length_req:\n",
    "            test_data.drop(x, inplace=True)\n",
    "\n",
    "    print(\"We have {:,} records > {} characters long\".format(test_data.shape[0], length_req))\n",
    "\n",
    "    # Only keep the unique article rows and their values\n",
    "    test_data.drop_duplicates(\"article\", keep='first', inplace=True)\n",
    "\n",
    "    print(\"{:,} Records are unique\".format(\n",
    "        test_data.shape[0]))\n",
    "\n",
    "    test_data['date'] = test_data['date'].str.replace(',', '')\n",
    "    \n",
    "    # This is a more clean and thorough url decoding function for decoding any character string...\n",
    "    test_data['article'] = test_data['article'].astype(str).apply(lambda x: html.unescape(x))\n",
    "\n",
    "    #test_data.head()\n",
    "    \n",
    "    # Check for null values\n",
    "    #print(test_data.isnull().sum(axis=0))\n",
    "    \n",
    "    # If there were null values, the below will replace them. - Sometimes dates are missing when transferred over.\n",
    "    test_data['date'].fillna(\"\", inplace=True)\n",
    "    #print(test_data.isnull().sum(axis=0))\n",
    "    \n",
    "    ## All pole entries - Have a look at how confident the model is on each individual entry.\n",
    "    ## Confidence level - State how many pole entries you want to see that the model has classified above a specific confidence level\n",
    "    confident_entries = 0\n",
    "\n",
    "    test_data['pole'] = 0  # Make a column 'pole', assign a value of 0 to indicate left articles\n",
    "\n",
    "    # predicted - should be an array of the predictions of the model in order that the articles come in.\n",
    "    predicted = text_clf.predict(test_data['article'])\n",
    "    class_probabilities = text_clf.predict_proba(test_data['article'])\n",
    "\n",
    "    for x in range(test_data.shape[0]):\n",
    "        if class_probabilities[x][0] > confidence_level:\n",
    "            confident_entries += 1\n",
    "\n",
    "    ## Pole entries above the chosen confidence level\n",
    "    print(\"Number of entries above %.2f confidence %s: %i / %i\" %(confidence_level, \"(see confidence_level)\", confident_entries, test_data.shape[0]))\n",
    "\n",
    "    # Accuracy\n",
    "    acc = metrics.accuracy_score(test_data.pole, predicted)\n",
    "    print(\"Entire dataset accuracy: {:.4f}\".format(acc), end='\\n\\n')\n",
    "    #print(\"Entire dataset accuracy:\", acc, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right Sources:\n",
      "NewsMax:\n",
      "We have 5,829 records\n",
      "We have 5,637 records > 800 characters long\n",
      "5,588 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 315 / 5588\n",
      "Entire dataset accuracy: 0.9315\n",
      "\n",
      "NationalReview:\n",
      "We have 619 records\n",
      "We have 605 records > 800 characters long\n",
      "578 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 299 / 578\n",
      "Entire dataset accuracy: 0.4394\n",
      "\n",
      "Redstate:\n",
      "We have 2,270 records\n",
      "We have 2,207 records > 800 characters long\n",
      "2,111 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 1170 / 2111\n",
      "Entire dataset accuracy: 0.4017\n",
      "\n",
      "TheBulwark:\n",
      "We have 1,413 records\n",
      "We have 1,405 records > 800 characters long\n",
      "578 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 388 / 578\n",
      "Entire dataset accuracy: 0.2924\n",
      "\n",
      "WashingtonExaminer:\n",
      "We have 1,117 records\n",
      "We have 1,104 records > 800 characters long\n",
      "983 Records are unique\n",
      "Number of entries above 0.53 confidence (see confidence_level): 310 / 983\n",
      "Entire dataset accuracy: 0.6419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now test on the right sources\n",
    "right_source = [\"NewsMax\", \"NationalReview\", \"Redstate\", \"TheBulwark\", \"WashingtonExaminer\"]\n",
    "\n",
    "print(\"Right Sources:\")\n",
    "for source in right_source:\n",
    "    test_data = pd.read_excel(\"csvs/\" + source + \".xlsx\", \n",
    "                  names=[\"date\", \"article\"])\n",
    "    \n",
    "    print(source + \":\")\n",
    "    print(\"We have {:,} records\".format(test_data.shape[0]))\n",
    "    \n",
    "    # Check for null values\n",
    "    #print(test_data.isnull().sum(axis=0))\n",
    "    \n",
    "    # If there were null values, the below will replace them. - Sometimes dates are missing when transferred over.\n",
    "    test_data['date'].fillna(\"\", inplace=True)\n",
    "    test_data['article'].fillna(\"\", inplace=True)\n",
    "    #print(test_data.isnull().sum(axis=0))\n",
    "\n",
    "    for x in range(test_data.shape[0]):\n",
    "        if len(test_data['article'][x]) < length_req:\n",
    "            test_data.drop(x, inplace=True)\n",
    "\n",
    "    print(\"We have {:,} records > {} characters long\".format(test_data.shape[0], length_req))\n",
    "\n",
    "    # Only keep the unique article rows and their values\n",
    "    test_data.drop_duplicates(\"article\", keep='first', inplace=True)\n",
    "\n",
    "    print(\"{:,} Records are unique\".format(\n",
    "        test_data.shape[0]))\n",
    "\n",
    "    test_data['date'] = test_data['date'].str.replace(',', '')\n",
    "    \n",
    "    # This is a more clean and thorough url decoding function for decoding any character string...\n",
    "    test_data['article'] = test_data['article'].astype(str).apply(lambda x: html.unescape(x))\n",
    "\n",
    "    #test_data.head()\n",
    "    \n",
    "    ## All pole entries - Have a look at how confident the model is on each individual entry.\n",
    "    ## Confidence level - State how many pole entries you want to see that the model has classified above a specific confidence level\n",
    "    confident_entries = 0\n",
    "\n",
    "    test_data['pole'] = 1  # Make a column 'pole', assign a value of 1 to indicate right articles\n",
    "\n",
    "    # predicted - should be an array of the predictions of the model in order that the articles come in.\n",
    "    predicted = text_clf.predict(test_data['article'])\n",
    "    class_probabilities = text_clf.predict_proba(test_data['article'])\n",
    "\n",
    "    for x in range(test_data.shape[0]):\n",
    "        if class_probabilities[x][0] > confidence_level:\n",
    "            confident_entries += 1\n",
    "\n",
    "    ## Pole entries above the chosen confidence level\n",
    "    print(\"Number of entries above %.2f confidence %s: %i / %i\" %(confidence_level, \"(see confidence_level)\", confident_entries, test_data.shape[0]))\n",
    "\n",
    "    # Accuracy\n",
    "    acc = metrics.accuracy_score(test_data.pole, predicted)\n",
    "    print(\"Entire dataset accuracy: {:.4f}\".format(acc), end='\\n\\n')\n",
    "    #print(\"Entire dataset accuracy:\", acc, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english')\n"
     ]
    }
   ],
   "source": [
    "# Access the classifier and CountVectorizer() in the Pipeline object from the .pkl file\n",
    "try:\n",
    "    clf = text_clf.named_steps['clf']\n",
    "except:\n",
    "    clf = text_clf.named_steps['eclf']\n",
    "count_vect = text_clf.named_steps['vect']\n",
    "print(clf)\n",
    "print(count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbott', 'abc', 'abc news', 'ability', 'able', 'abortion', 'abortion rights', 'abortions', 'abrams', 'abroad', 'absence', 'absentee', 'absentee ballot', 'absentee ballots', 'absentee voting', 'absolute', 'absolutely', 'abuse', 'abuse power', 'academy', 'accept', 'access', 'according', 'according hill', 'according new', 'according new york', 'according report', 'according washington', 'account', 'accountability', 'accountable', 'accounts', 'accurate', 'accusation', 'accusations', 'accused', 'accusing', 'achieve', 'acknowledge', 'acknowledged', 'acquit', 'acquittal', 'acquitted', 'act', 'acted', 'acting', 'action', 'actions', 'active', 'actively', 'activist', 'activists', 'activities', 'activity', 'acts', 'actual', 'actually', 'ad', 'adam', 'adam schiff', 'adams', 'add', 'added', 'adding', 'addition', 'additional', 'address', 'addressed', 'addressing', 'adequate', 'administration', 'administration official', 'administration officials', 'administration response', 'administration said', 'admit', 'admitted', 'admitting', 'adopted', 'ads', 'adult', 'adults', 'advance', 'advanced', 'advantage', 'advertising', 'advice', 'advised', 'adviser', 'adviser john', 'adviser john bolton', 'adviser michael', 'adviser michael flynn', 'advisers', 'advisory', 'advocacy', 'advocate', 'advocated', 'advocates', 'affairs', 'affairs committee', 'affect', 'affected', 'affiliate', 'afford', 'affordable', 'affordable care', 'affordable care act', 'afghanistan', 'afraid', 'african', 'african american', 'african americans', 'afternoon', 'age', 'agencies', 'agency', 'agenda', 'agent', 'agents', 'aggressive', 'ago', 'agree', 'agreed', 'agreement', 'agreements', 'agriculture', 'ahead', 'aid', 'aid ukraine', 'aide', 'aides', 'aim', 'aimed', 'aims', 'air', 'air force', 'aired', 'airing', 'airlines', 'airport', 'aisle', 'al', 'alabama', 'alan', 'alan dershowitz', 'alarm', 'alarming', 'alaska', 'alex', 'alex azar', 'alexander', 'alexandria', 'alexandria ocasio', 'alexandria ocasio cortez', 'ali', 'aligned', 'allegation', 'allegations', 'alleged', 'allegedly', 'allergy', 'allergy infectious', 'allergy infectious diseases', 'allies', 'allow', 'allowed', 'allowing', 'allows', 'ally', 'alongside', 'alternative', 'amash', 'amazon', 'ambassador', 'amendment', 'america', 'america newsroom', 'american', 'american civil', 'american history', 'american people', 'american public', 'americans', 'americans say', 'amid', 'amid coronavirus', 'amid coronavirus pandemic', 'amid pandemic', 'amounts', 'amy', 'amy klobuchar', 'amy klobuchar minn', 'analysis', 'analyst', 'anchor', 'anderson', 'anderson cooper', 'andrew', 'andrew cuomo', 'andrew yang', 'andy', 'angeles', 'anger', 'angry', 'announce', 'announced', 'announcement', 'announcing', 'annual', 'anonymity', 'answer', 'answer qunited', 'answered', 'answers', 'anthony', 'anthony fauci', 'anti', 'anti racism', 'antifa', 'anybody', 'anymore', 'aoc', 'ap', 'apart', 'apologize', 'apologized', 'apology', 'app', 'apparent', 'apparently', 'appeal', 'appeals', 'appeals court', 'appear', 'appearance', 'appearances', 'appeared', 'appearing', 'appears', 'application', 'applications', 'applied', 'apply', 'appointed', 'appointment', 'approach', 'appropriate', 'approval', 'approve', 'approved', 'april', 'arabia', 'area', 'areas', 'arena', 'argue', 'argued', 'arguing', 'argument', 'arguments', 'argunited', 'arizona', 'arkansas', 'arm', 'armed', 'arms', 'army', 'arrest', 'arrested', 'arrests', 'arrived', 'article', 'articles', 'articles impeachment', 'asian', 'aside', 'ask', 'asked', 'asking', 'asks', 'assassination', 'assault', 'assaulted', 'assembly', 'assertion', 'assessment', 'assets', 'assistance', 'assistant', 'associate', 'associated', 'associated press', 'associates', 'association', 'assume', 'asylum', 'atlanta', 'atlantic', 'attack', 'attacked', 'attacking', 'attacks', 'attempt', 'attempted', 'attempting', 'attempts', 'attend', 'attendance', 'attended', 'attendees', 'attending', 'attention', 'attorney', 'attorney general', 'attorney general william', 'attorneys', 'audience', 'audio', 'aug', 'august', 'author', 'authoritarian', 'authorities', 'authority', 'authorization', 'authorized', 'autonomous', 'autonomous zone', 'available', 'average', 'avoid', 'avoided', 'aware', 'away', 'axios', 'azar', 'backed', 'backers', 'background', 'backing', 'backlash', 'backs', 'bad', 'badly', 'baghdad', 'bail', 'bailout', 'balance', 'ballot', 'ballots', 'ban', 'bank', 'bankruptcy', 'banks', 'banned', 'banning', 'bans', 'bar', 'barack', 'barack obama', 'barr', 'barr said', 'bars', 'base', 'based', 'bases', 'basic', 'basically', 'basis', 'battle', 'battleground', 'battleground states', 'bay', 'beach', 'bear', 'beast', 'beat', 'beating', 'beds', 'began', 'begin', 'beginning', 'begins', 'begun', 'behalf', 'behavior', 'beijing', 'belief', 'beliefs', 'believe', 'believed', 'believes', 'ben', 'bend', 'bend indiana', 'bend indiana mayor', 'benefit', 'benefits', 'berman', 'bernie', 'bernie sanders', 'bernie sanders vt', 'best', 'better', 'bias', 'bible', 'bid', 'biden', 'biden campaign', 'biden president', 'biden said', 'biden sanders', 'biden son', 'biden son hunter', 'biden told', 'bidens', 'big', 'bigger', 'biggest', 'billion', 'billionaire', 'billionaires', 'billions', 'bills', 'bipartisan', 'birds', 'birth', 'birx', 'bit', 'black', 'black americans', 'black community', 'black lives', 'black lives matter', 'black man', 'black people', 'black voters', 'blame', 'blamed', 'blaming', 'blasio', 'blasted', 'block', 'blocked', 'blocking', 'blocks', 'blog', 'blood', 'bloomberg', 'bloomberg campaign', 'blow', 'blue', 'blunt', 'board', 'bob', 'body', 'bold', 'bolster', 'bolton', 'bolton book', 'bolton said', 'boogaloo', 'book', 'booker', 'books', 'boost', 'border', 'border patrol', 'borders', 'born', 'boss', 'boston', 'bottoms', 'bought', 'bounties', 'bowman', 'box', 'brad', 'branch', 'break', 'breaking', 'breathe', 'brennan', 'brett', 'brian', 'bridge', 'brief', 'briefed', 'briefing', 'briefings', 'brien', 'bright', 'bring', 'bringing', 'british', 'broad', 'broadcast', 'broader', 'broke', 'broken', 'bronx', 'brooklyn', 'brother', 'brought', 'brown', 'brutality', 'budget', 'budgets', 'build', 'building', 'buildings', 'built', 'bullets', 'burden', 'bureau', 'burr', 'bush', 'business', 'businesses', 'businessman', 'buttigieg', 'buy', 'buying', 'cabinet', 'cable', 'cable homes', 'cable homes directv', 'cable suddenlink', 'cable suddenlink ch', 'cable systems', 'cable systems click', 'calif', 'california', 'called', 'calling', 'calls', 'calm', 'came', 'camera', 'camp', 'campaign', 'campaign finance', 'campaign manager', 'campaign rally', 'campaign said', 'campaign trail', 'campaigning', 'campaigns', 'camps', 'campus', 'canada', 'cancel', 'canceled', 'cancer', 'candidacy', 'candidate', 'candidates', 'capable', 'capacity', 'capital', 'capitol', 'capitol hill', 'captain', 'captured', 'car', 'care', 'care act', 'care workers', 'career', 'careful', 'carefully', 'cares', 'cares act', 'carlson', 'carolina', 'carried', 'carried million', 'carried million cable', 'carry', 'carrying', 'cars', 'carson', 'carter', 'case', 'cases', 'cases covid', 'cases deaths', 'cases united', 'cases united states', 'cash', 'cast', 'castro', 'catch', 'catholic', 'caucus', 'caucuses', 'caught', 'cause', 'caused', 'caused coronavirus', 'causes', 'causing', 'caution', 'cbs', 'cbs news', 'cdc', 'celebrate', 'celebration', 'census', 'center', 'centers', 'centers disease', 'centers disease control', 'central', 'century', 'ceo', 'ceremony', 'certain', 'certainly', 'ch', 'ch cable', 'ch cable systems', 'ch cox', 'ch cox cable', 'ch dish', 'ch dish network', 'ch fios', 'ch fios ch', 'ch mediacom', 'ch mediacom ch', 'ch optimum', 'ch optimum ch', 'ch spectrum', 'ch spectrum verse', 'ch xfinity', 'ch xfinity ch', 'chad', 'chad wolf', 'chain', 'chair', 'chairman', 'chairs', 'challenge', 'challenged', 'challenger', 'challenges', 'challenging', 'chamber', 'chance', 'chances', 'change', 'changed', 'changes', 'changing', 'channel', 'chaos', 'character', 'characterized', 'charge', 'charged', 'charges', 'charles', 'charlotte', 'chauvin', 'check', 'check clip', 'checks', 'cheney', 'chicago', 'chief', 'chief justice', 'chief justice john', 'chief staff', 'chiefs', 'child', 'child care', 'childhood', 'children', 'china', 'chinese', 'chinese communist', 'chinese communist party', 'chinese government', 'choice', 'choose', 'choosing', 'chose', 'chosen', 'chris', 'christian', 'christians', 'christopher', 'chuck', 'chuck schumer', 'church', 'churches', 'cia', 'cipollone', 'circuit', 'circunited', 'circunited statesances', 'cisneros', 'cited', 'cities', 'citing', 'citizen', 'citizens', 'city', 'city council', 'city mayor', 'civil', 'civil liberties', 'civil liberties union', 'civil rights', 'civil war', 'claim', 'claimed', 'claiming', 'claims', 'clarity', 'class', 'classes', 'classified', 'classified information', 'clean', 'cleaning', 'clear', 'cleared', 'clearly', 'click', 'climate', 'climate change', 'clinic', 'clinics', 'clinton', 'clip', 'close', 'closed', 'closely', 'closer', 'closest', 'closing', 'closures', 'club', 'clyburn', 'cnbc', 'cnn', 'cnn reported', 'cnn state', 'cnn state union', 'coach', 'coalition', 'coast', 'cohen', 'cold', 'collapse', 'colleagunited', 'collect', 'collective', 'college', 'colleges', 'collins', 'collins maine', 'collins said', 'color', 'colorado', 'columbia', 'column', 'columnist', 'com', 'combat', 'combined', 'come', 'comeback', 'comes', 'comey', 'comfortable', 'coming', 'command', 'commander', 'comment', 'commentator', 'commented', 'comments', 'comments came', 'commerce', 'commercial', 'commission', 'commissioner', 'commit', 'commitment', 'committed', 'committee', 'committee chairman', 'committees', 'common', 'communication', 'communications', 'communications director', 'communist', 'communist party', 'communities', 'community', 'companies', 'company', 'compared', 'comparison', 'compete', 'competing', 'competitive', 'complained', 'complaint', 'complaints', 'complete', 'completed', 'completely', 'complex', 'complicated', 'comply', 'comprehensive', 'compromise', 'concern', 'concerned', 'concerning', 'concerns', 'concluded', 'conclusion', 'condemned', 'condition', 'condition anonymity', 'conditions', 'conduct', 'conducted', 'conducting', 'confederate', 'confederate flag', 'conference', 'confidence', 'confident', 'confirm', 'confirmation', 'confirmed', 'confirmed cases', 'conflict', 'confusion', 'congress', 'congressional', 'congressional district', 'congressman', 'congresswoman', 'connected', 'connecticut', 'connection', 'consensus', 'consent', 'consequences', 'conservative', 'conservatives', 'consider', 'consideration', 'considered', 'considering', 'considers', 'consistent', 'consistently', 'conspiracy', 'conspiracy theories', 'conspiracy theory', 'constituents', 'constitution', 'constitutional', 'construction', 'consultant', 'consumer', 'contact', 'contain', 'contained', 'contender', 'contenders', 'content', 'contest', 'contests', 'context', 'continue', 'continued', 'continuing', 'continunited', 'continunited statesto', 'contract', 'contracted', 'contracting', 'contrast', 'contributed', 'contributed report', 'contributions', 'control', 'control prevention', 'controlled', 'controlled senate', 'controversial', 'controversy', 'convention', 'conversation', 'conversations', 'convict', 'convicted', 'conviction', 'conway', 'cook', 'cooper', 'cooperation', 'coordinated', 'coordinator', 'cop', 'cops', 'copy', 'core', 'cornyn', 'coronavirus', 'coronavirus cases', 'coronavirus crisis', 'coronavirus outbreak', 'coronavirus pandemic', 'coronavirus relief', 'coronavirus response', 'coronavirus stimulus', 'coronavirus task', 'coronavirus task force', 'coronavirus testing', 'corporate', 'corporations', 'corps', 'correct', 'correction', 'correspondent', 'corrupt', 'corruption', 'cortez', 'cory', 'cory booker', 'cost', 'costs', 'cotton', 'council', 'counited', 'counited states', 'counited statesl', 'counited statesl robert', 'counited stateslor', 'count', 'counted', 'counter', 'counties', 'countries', 'country', 'country said', 'counts', 'county', 'couple', 'court', 'court appeals', 'court decision', 'court justice', 'court ruling', 'courthouse', 'courts', 'cover', 'coverage', 'covered', 'covering', 'coverings', 'covid', 'covid cases', 'covid disease', 'covid disease caused', 'covid pandemic', 'cox', 'cox cable', 'cox cable suddenlink', 'crazy', 'create', 'created', 'creates', 'creating', 'creation', 'credibility', 'credible', 'credit', 'crew', 'crime', 'crimes', 'criminal', 'criminal justice', 'criminal justice reform', 'criminals', 'crises', 'crisis', 'critic', 'critical', 'criticism', 'criticize', 'criticized', 'criticizing', 'critics', 'cross', 'crowd', 'crowded', 'crowds', 'crozier', 'crucial', 'crunited', 'crunited states', 'cruz', 'cuban', 'cuellar', 'cultural', 'culture', 'cunited', 'cunited states', 'cuomo', 'cuomo said', 'curb', 'cure', 'curfew', 'current', 'currently', 'curve', 'custody', 'customers', 'customs', 'cut', 'cutting', 'cycle', 'daca', 'daily', 'dakota', 'dallas', 'damage', 'damaging', 'dan', 'danger', 'dangerous', 'daniel', 'dark', 'data', 'date', 'daughter', 'david', 'davis', 'day', 'days', 'days later', 'dead', 'deadline', 'deadly', 'deal', 'dealing', 'dealings', 'deals', 'death', 'death george', 'death george floyd', 'death toll', 'deaths', 'debate', 'debate stage', 'debates', 'deborah', 'deborah birx', 'debt', 'decade', 'decades', 'december', 'decide', 'decided', 'decision', 'decisions', 'declaration', 'declare', 'declared', 'declaring', 'decline', 'declined', 'declined comment', 'dedicated', 'deemed', 'deep', 'deeply', 'defeat', 'defeated', 'defend', 'defended', 'defending', 'defense', 'defense secretary', 'defense secretary mark', 'defense team', 'deficit', 'definitely', 'defund', 'defund police', 'defunding', 'defunding police', 'degree', 'delaware', 'delay', 'delayed', 'delays', 'delegate', 'delegates', 'deliver', 'delivered', 'delivering', 'delivery', 'dem', 'demand', 'demanded', 'demanding', 'demands', 'demings', 'democracy', 'democrat', 'democrat said', 'democratic', 'democratic candidates', 'democratic debate', 'democratic national', 'democratic national committee', 'democratic nomination', 'democratic nominee', 'democratic party', 'democratic presidential', 'democratic presidential candidate', 'democratic presidential nomination', 'democratic presidential nominee', 'democratic presidential primary', 'democratic primary', 'democratic socialist', 'democratic voters', 'democrats', 'democrats republicans', 'democrats said', 'democrats want', 'demonstrated', 'demonstrations', 'demonstrators', 'dems', 'denied', 'denounced', 'deny', 'denying', 'department', 'department health', 'department health human', 'department homeland', 'department homeland security', 'department justice', 'department said', 'departments', 'departure', 'depend', 'deploy', 'deployed', 'deportation', 'depression', 'deputy', 'dershowitz', 'des', 'des moines', 'desantis', 'described', 'describing', 'deserve', 'designed', 'desire', 'desk', 'desperate', 'despite', 'destroy', 'destroying', 'destruction', 'detailed', 'details', 'detained', 'detainees', 'detention', 'determine', 'determined', 'detroit', 'devastating', 'develop', 'developed', 'developing', 'development', 'devos', 'dewine', 'dhs', 'diagnosed', 'dick', 'did', 'did immediately', 'did immediately respond', 'did know', 'did respond', 'did respond requnited', 'did say', 'did want', 'die', 'died', 'difference', 'differences', 'different', 'differently', 'difficult', 'digital', 'dinner', 'dire', 'direct', 'directed', 'direction', 'directly', 'director', 'director national', 'director national intelligence', 'directv', 'directv ch', 'directv ch dish', 'disabilities', 'disability', 'disagree', 'disappointed', 'disappointing', 'disaster', 'disclose', 'disclosure', 'discretion', 'discrimination', 'discunited', 'discunited statesd', 'discunited statesng', 'discunited stateson', 'discunited statesons', 'discunited statesthe', 'disease', 'disease caused', 'disease caused coronavirus', 'disease control', 'disease control prevention', 'disease expert', 'diseases', 'dish', 'dish network', 'dish network ch', 'disinfectant', 'disinformation', 'dismiss', 'dismissal', 'dismissed', 'display', 'disproportionately', 'dispute', 'distance', 'distancing', 'distancing guidelines', 'distancing measures', 'distributed', 'district', 'district attorney', 'district columbia', 'district court', 'district judge', 'districts', 'disturbing', 'diverse', 'diversity', 'divide', 'divided', 'division', 'divisive', 'dnc', 'dobbs', 'doctor', 'doctors', 'document', 'documents', 'does', 'does mean', 'does think', 'does want', 'dog', 'doing', 'doj', 'dollar', 'dollars', 'domestic', 'domestic violence', 'don', 'donald', 'donations', 'donor', 'donors', 'door', 'doors', 'double', 'doubt', 'doug', 'doug collins', 'downplayed', 'downplaying', 'downtown', 'dozen', 'dozens', 'dr', 'dr anthony', 'dr anthony fauci', 'dr fauci', 'draft', 'dramatic', 'dramatically', 'draw', 'drawing', 'drawn', 'dream', 'dreamers', 'drew', 'drive', 'driven', 'driver', 'drivers', 'driving', 'drop', 'dropped', 'dropping', 'drug', 'drunited', 'drunited states', 'durham', 'duty', 'dying', 'eager', 'earlier', 'earlier day', 'earlier month', 'earlier week', 'earlier year', 'early', 'early voting', 'earn', 'earned', 'earning', 'ease', 'easier', 'easily', 'east', 'easter', 'easy', 'ebola', 'echoed', 'economic', 'economic adviser', 'economic recovery', 'economies', 'economist', 'economy', 'ed', 'edge', 'editor', 'editorial', 'education', 'effect', 'effective', 'effectively', 'effects', 'effort', 'efforts', 'el', 'elderly', 'elect', 'elected', 'elected officials', 'election', 'election campaign', 'election day', 'election officials', 'election year', 'elections', 'electoral', 'electorate', 'eligible', 'eliminate', 'elizabeth', 'elizabeth warren', 'elizabeth warren mass', 'email', 'emails', 'embassy', 'embrace', 'emerge', 'emerged', 'emergency', 'emissions', 'emphasized', 'employed', 'employee', 'employees', 'employer', 'employers', 'employment', 'enact', 'enacted', 'encourage', 'encouraged', 'encouraging', 'end', 'end year', 'ended', 'ending', 'endorse', 'endorsed', 'endorsement', 'endorsements', 'endorsing', 'ends', 'enemy', 'energy', 'enforce', 'enforcement', 'enforcement officers', 'engage', 'engaged', 'engaging', 'engel', 'english', 'enhanced', 'enormous', 'ensure', 'ensuring', 'enter', 'entered', 'entering', 'enthusiasm', 'enthusiastic', 'entire', 'entirely', 'entities', 'entry', 'environment', 'environmental', 'epa', 'epicenter', 'epidemic', 'equal', 'equality', 'equipment', 'era', 'eric', 'ernst', 'error', 'error plus', 'error plus minus', 'errors', 'escalation', 'especially', 'esper', 'essential', 'essentially', 'establish', 'established', 'establishment', 'estate', 'estimated', 'estimates', 'ethics', 'europe', 'european', 'evangelical', 'evangelicals', 'evening', 'event', 'events', 'eventually', 'evers', 'everybody', 'eviction', 'evictions', 'evidence', 'ex', 'exactly', 'examiner', 'example', 'excessive', 'exchange', 'excuse', 'executive', 'executive director', 'executive order', 'executives', 'exercise', 'exist', 'existing', 'exit', 'expand', 'expanded', 'expanding', 'expansion', 'expect', 'expected', 'expects', 'expenses', 'expensive', 'experience', 'experienced', 'experiencing', 'expert', 'experts', 'experts say', 'explain', 'explained', 'explaining', 'explanation', 'explicitly', 'exposed', 'exposure', 'express', 'expressed', 'extend', 'extended', 'extending', 'extension', 'extensive', 'extent', 'extra', 'extraordinary', 'extreme', 'extremely', 'eye', 'eyes', 'face', 'face coverings', 'face mask', 'face masks', 'facebook', 'faced', 'faces', 'facial', 'facilities', 'facility', 'facing', 'fact', 'factor', 'factors', 'facts', 'failed', 'failing', 'failure', 'failures', 'fair', 'fairly', 'faith', 'fake', 'fake news', 'fall', 'fallen', 'falling', 'fallout', 'false', 'falsely', 'familiar', 'families', 'family', 'family members', 'fans', 'far', 'far left', 'far right', 'farm', 'farmers', 'fast', 'faster', 'father', 'fauci', 'fauci said', 'favor', 'favorable', 'favored', 'favorite', 'fbi', 'fbi director', 'fda', 'fear', 'fears', 'featured', 'features', 'feb', 'february', 'fed', 'federal', 'federal agents', 'federal court', 'federal government', 'federal judge', 'federal law', 'federal law enforcement', 'federal officers', 'federal prosecutors', 'federal reserve', 'federation', 'feednetwork', 'feel', 'feel like', 'feeling', 'feels', 'feet', 'fell', 'fellow', 'felt', 'fema', 'female', 'fever', 'fewer', 'field', 'fifth', 'fight', 'fighting', 'figure', 'figures', 'file', 'filed', 'filing', 'filled', 'film', 'final', 'finally', 'finance', 'financial', 'finding', 'findings', 'finds', 'fine', 'finish', 'finished', 'fios', 'fios ch', 'fios ch optimum', 'fired', 'fires', 'fireworks', 'firing', 'firm', 'firms', 'fisa', 'fiscal', 'fit', 'fix', 'fla', 'flag', 'flags', 'flight', 'flights', 'floor', 'florida', 'floyd', 'floyd black', 'floyd black man', 'floyd death', 'flu', 'fly', 'flynn', 'focus', 'focused', 'focusing', 'folks', 'follow', 'followed', 'followers', 'following', 'following death', 'follows', 'food', 'footage', 'football', 'forbes', 'force', 'forced', 'forces', 'forcing', 'ford', 'foreign', 'foreign affairs', 'foreign policy', 'forget', 'form', 'formal', 'formally', 'forms', 'fort', 'forth', 'fortune', 'forum', 'forward', 'fossil', 'fossil fuel', 'fought', 'foundation', 'founded', 'founder', 'fourth', 'fox', 'fox business', 'fox friends', 'fox news', 'fox news america', 'fox news fox', 'fox news host', 'fox news sunday', 'francisco', 'frankly', 'fraud', 'fred', 'free', 'freedom', 'frequent', 'frequently', 'fresh', 'friday', 'friday morning', 'friday night', 'friend', 'friendly', 'friends', 'frisk', 'frustrated', 'fuel', 'fueled', 'fully', 'fund', 'fundamental', 'funded', 'funding', 'fundraiser', 'fundraisers', 'fundraising', 'funds', 'funeral', 'future', 'ga', 'gabbard', 'gaetz', 'gain', 'gained', 'gains', 'game', 'gang', 'gap', 'garcia', 'garden', 'gardner', 'gas', 'gates', 'gathered', 'gathering', 'gatherings', 'gave', 'gavin', 'gavin newsom', 'gay', 'gear', 'gen', 'gen qassem', 'gen qassem soleimani', 'gender', 'general', 'general election', 'general william', 'general william barr', 'generally', 'generation', 'george', 'george bush', 'george floyd', 'george floyd black', 'george washington', 'georgia', 'germany', 'gets', 'getting', 'giant', 'gingrich', 'girl', 'giuliani', 'given', 'gives', 'giving', 'global', 'global coronavirus', 'global coronavirus pandemic', 'global health', 'global pandemic', 'globe', 'gloves', 'goal', 'goals', 'god', 'goes', 'going', 'going happen', 'golf', 'gone', 'gonna', 'good', 'good job', 'goods', 'google', 'gop', 'gop senators', 'gorsuch', 'got', 'gotten', 'gov', 'gov andrew', 'gov andrew cuomo', 'gov gavin', 'gov gavin newsom', 'gov gretchen', 'gov gretchen whitmer', 'gov ron', 'gov ron desantis', 'government', 'government response', 'governments', 'governor', 'governors', 'goya', 'graham', 'graham said', 'grand', 'grant', 'granted', 'grants', 'grassley', 'grassroots', 'grave', 'great', 'greater', 'greatest', 'green', 'green new', 'green new deal', 'greg', 'greg kelly', 'greg kelly reports', 'grenell', 'gretchen', 'gretchen whitmer', 'grew', 'grisham', 'grocery', 'gross', 'ground', 'grounds', 'grounited', 'grounited states', 'grounited stateshave', 'group', 'grow', 'growing', 'grown', 'growth', 'guarantee', 'guard', 'guidance', 'guide', 'guide coronavirus', 'guidelines', 'guilty', 'gun', 'gun control', 'gunited', 'gunited states', 'gunited statess', 'guy', 'hailed', 'hair', 'half', 'hall', 'halt', 'hampshire', 'hampshire primary', 'hand', 'hand sanitizer', 'handed', 'handful', 'handle', 'handled', 'handling', 'handling coronavirus', 'hands', 'hannity', 'happen', 'happened', 'happening', 'happens', 'happy', 'harassment', 'hard', 'harder', 'hardest', 'harm', 'harris', 'harsh', 'harvard', 'hate', 'haul', 'having', 'hawaii', 'hawley', 'hayes', 'head', 'headed', 'heading', 'headquarters', 'heads', 'health', 'health care', 'health care workers', 'health crisis', 'health emergency', 'health experts', 'health human', 'health human services', 'health insurance', 'health officials', 'health organization', 'health safety', 'healthcare', 'healthy', 'hear', 'heard', 'hearing', 'hearings', 'heart', 'heat', 'heavily', 'heavy', 'height', 'held', 'hell', 'help', 'helped', 'helping', 'helps', 'henry', 'heritage', 'heroes', 'hewill', 'hhs', 'high', 'high profile', 'high risk', 'high school', 'higher', 'highest', 'highlighted', 'highly', 'hill', 'hill reported', 'hillary', 'hillary clinton', 'hire', 'hired', 'hispanic', 'historic', 'historical', 'historically', 'history', 'hit', 'hits', 'hitting', 'hoax', 'hogan', 'hold', 'holding', 'holds', 'holiday', 'home', 'home order', 'home orders', 'home state', 'homeland', 'homeland security', 'homeless', 'homelessness', 'homes', 'homes directv', 'homes directv ch', 'honest', 'hong', 'hong kong', 'honor', 'honored', 'hope', 'hoped', 'hopeful', 'hopefully', 'hopes', 'hoping', 'hopkins', 'horowitz', 'horrible', 'hospital', 'hospitals', 'host', 'hosted', 'hosts', 'hot', 'hotel', 'hotels', 'hotline', 'hounited', 'hounited states', 'hounited statesafter', 'hour', 'house', 'house chief', 'house chief staff', 'house coronavirus', 'house coronavirus task', 'house correspondent', 'house counited', 'house democrats', 'house impeachment', 'house intelligence', 'house intelligence committee', 'house judiciary', 'house judiciary committee', 'house minority', 'house official', 'house officials', 'house press', 'house press secretary', 'house representatives', 'house said', 'house senate', 'house speaker', 'house speaker nancy', 'households', 'houses', 'housing', 'houston', 'huge', 'human', 'human rights', 'human services', 'humanitarian', 'hundreds', 'hundreds thousands', 'hunt', 'hunter', 'hunter biden', 'hurricane', 'hurt', 'husband', 'hydroxychloroquine', 'ice', 'idaho', 'idea', 'ideas', 'identified', 'identify', 'identity', 'ideological', 'ignore', 'ignored', 'ignoring', 'ihave', 'ii', 'ill', 'illegal', 'illegally', 'illinois', 'illness', 'image', 'images', 'imagine', 'immediate', 'immediately', 'immediately respond', 'immediately respond requnited', 'immigrant', 'immigrants', 'immigration', 'imminent', 'immune', 'immunity', 'impact', 'impacts', 'impeach', 'impeachable', 'impeached', 'impeachment', 'impeachment inquiry', 'impeachment managers', 'impeachment trial', 'impeachment trial president', 'implement', 'implemented', 'importance', 'important', 'important tv', 'important tv carried', 'impose', 'imposed', 'impossible', 'improve', 'inaccurate', 'inappropriate', 'incident', 'incidents', 'include', 'included', 'includes', 'including', 'income', 'increase', 'increased', 'increases', 'increasing', 'increasingly', 'incredible', 'incredibly', 'incumbent', 'independence', 'independent', 'independents', 'india', 'indian', 'indiana', 'indiana mayor', 'indiana mayor pete', 'indicated', 'individual', 'individuals', 'indoor', 'industries', 'industry', 'inequality', 'infected', 'infection', 'infections', 'infectious', 'infectious disease', 'infectious disease expert', 'infectious diseases', 'influence', 'influential', 'information', 'informed', 'infrastructure', 'ingraham', 'inhofe', 'initial', 'initially', 'initiative', 'injured', 'injuries', 'injustice', 'inmates', 'inquiry', 'inside', 'insider', 'insisted', 'insisting', 'inslee', 'inspector', 'inspector general', 'instagram', 'instance', 'instances', 'instead', 'institute', 'institute allergy', 'institute allergy infectious', 'institution', 'institutions', 'insurance', 'integrity', 'intelligence', 'intelligence committee', 'intelligence community', 'intelligence officials', 'intended', 'intent', 'interested', 'interests', 'interfere', 'interference', 'interior', 'internal', 'international', 'internet', 'interview', 'interview fox', 'interview fox news', 'interviews', 'introduced', 'invest', 'investigate', 'investigated', 'investigating', 'investigation', 'investigations', 'investigators', 'investment', 'investments', 'invited', 'involved', 'involvement', 'involving', 'iowa', 'iowa caucuses', 'iowa democratic', 'iowa democratic party', 'iowa new', 'iowa new hampshire', 'iran', 'iranian', 'iraq', 'iraqi', 'irresponsible', 'irs', 'islamic', 'island', 'isolation', 'israel', 'israeli', 'issue', 'issued', 'issuing', 'issunited', 'issunited states', 'italy', 'items', 'ivanka', 'iwill', 'jackson', 'jacksonville', 'jail', 'james', 'jan', 'january', 'jared', 'jared kushner', 'jay', 'jayapal', 'jeff', 'jeff sessions', 'jerry', 'jersey', 'jesus', 'jewish', 'jews', 'jim', 'jinping', 'job', 'jobless', 'jobs', 'joe', 'joe biden', 'john', 'john bolton', 'john roberts', 'johns', 'johns hopkins', 'johnson', 'join', 'joined', 'joining', 'joint', 'joke', 'jonathan', 'jones', 'jordan', 'josh', 'journal', 'journalist', 'journalists', 'jr', 'judge', 'judges', 'judgment', 'judicial', 'judiciary', 'judiciary committee', 'july', 'june', 'juneteenth', 'jurors', 'jury', 'just', 'just days', 'just like', 'justice', 'justice department', 'justice john', 'justice john roberts', 'justice reform', 'justices', 'justify', 'justin', 'kamala', 'kamala harris', 'kansas', 'karen', 'kasich', 'katie', 'kavanaugh', 'kayleigh', 'kayleigh mcenany', 'keeping', 'keeps', 'kelly', 'kelly reports', 'kellyanne', 'kellyanne conway', 'kemp', 'ken', 'kennedy', 'kentucky', 'kept', 'kevin', 'kevin mccarthy', 'key', 'kids', 'kill', 'killed', 'killed people', 'killing', 'killing george', 'killing george floyd', 'kim', 'kind', 'kinds', 'king', 'kits', 'klobuchar', 'klobuchar minn', 'knee', 'kneeling', 'knelt', 'knew', 'know', 'knowing', 'knowledge', 'known', 'knows', 'kong', 'korea', 'korean', 'kudlow', 'kudlow said', 'kushner', 'ky', 'la', 'lab', 'labor', 'labs', 'lack', 'lady', 'lafayette', 'lago', 'laid', 'lamented', 'land', 'language', 'large', 'largely', 'larger', 'largest', 'larry', 'larry kudlow', 'las', 'las vegas', 'lashed', 'late', 'late march', 'later', 'latest', 'latino', 'latinos', 'launch', 'launched', 'launching', 'laura', 'law', 'law enforcement', 'law enforcement officers', 'law order', 'law professor', 'lawmaker', 'lawmakers', 'laws', 'lawsuit', 'lawsuits', 'lawyer', 'lawyers', 'lay', 'layoffs', 'lead', 'leader', 'leader chuck', 'leader chuck schumer', 'leader kevin', 'leader kevin mccarthy', 'leader mitch', 'leader mitch mcconnell', 'leaders', 'leadership', 'leading', 'leads', 'league', 'leaked', 'leaning', 'learn', 'learned', 'learning', 'leave', 'leaves', 'leaving', 'led', 'lee', 'left', 'left wing', 'legacy', 'legal', 'legal team', 'legally', 'legislation', 'legislative', 'legislature', 'legitimate', 'lemon', 'lengthy', 'let', 'lethal', 'letter', 'letters', 'level', 'levels', 'leverage', 'lewis', 'lgbtq', 'liability', 'liberal', 'liberties', 'liberties union', 'liberty', 'lie', 'lied', 'lies', 'life', 'lift', 'lifted', 'light', 'like', 'likely', 'likes', 'limbaugh', 'limit', 'limited', 'limiting', 'limits', 'lincoln', 'lincoln project', 'lindsey', 'lindsey graham', 'line', 'lines', 'linick', 'link', 'linked', 'lisa', 'list', 'listed', 'listen', 'listening', 'literally', 'litigation', 'little', 'little bit', 'live', 'lived', 'lives', 'lives matter', 'lives matter movement', 'living', 'loan', 'loans', 'local', 'local governments', 'local officials', 'located', 'location', 'locations', 'lockdown', 'locked', 'loeffler', 'logan', 'long', 'long term', 'long time', 'longer', 'longtime', 'look', 'look like', 'looked', 'looking', 'looks', 'looks like', 'looting', 'los', 'los angeles', 'lose', 'losing', 'loss', 'losses', 'lost', 'lot', 'lot people', 'lounited', 'lounited statesana', 'lounited statesille', 'love', 'loved', 'low', 'low income', 'lower', 'lt', 'lunch', 'luther', 'lying', 'lying congress', 'lying fbi', 'maddow', 'maduro', 'magazine', 'mail', 'mail ballots', 'mail voting', 'main', 'maine', 'mainstream', 'maintain', 'maintained', 'major', 'majority', 'majority leader', 'majority leader mitch', 'make', 'make sure', 'makes', 'making', 'making sure', 'man', 'manage', 'managed', 'management', 'manager', 'managers', 'manchin', 'mandate', 'mandatory', 'manhattan', 'manner', 'manufacturers', 'manufacturing', 'manuscript', 'map', 'mar', 'mar lago', 'march', 'marco', 'marco rubio', 'margin', 'margin error', 'margin error plus', 'maria', 'marine', 'mark', 'mark esper', 'mark meadows', 'marked', 'market', 'markets', 'marks', 'married', 'martin', 'mary', 'maryland', 'mask', 'masks', 'mass', 'massachusetts', 'massie', 'massive', 'match', 'mate', 'material', 'materials', 'matt', 'matter', 'matter movement', 'matters', 'mattis', 'maybe', 'mayor', 'mayor blasio', 'mayor pete', 'mayor pete buttigieg', 'mayors', 'mccain', 'mccarthy', 'mccloskey', 'mcconnell', 'mcconnell ky', 'mcconnell said', 'mcdaniel', 'mcenany', 'mcenany said', 'meadows', 'meadows said', 'mean', 'meaning', 'means', 'meant', 'measure', 'measures', 'media', 'mediacom', 'mediacom ch', 'mediacom ch cable', 'medicaid', 'medical', 'medical equipment', 'medical supplies', 'medicare', 'medicine', 'meet', 'meet press', 'meeting', 'meetings', 'member', 'members', 'members congress', 'membership', 'memo', 'memoir', 'memorial', 'memorial day', 'men', 'men women', 'mental', 'mental health', 'mention', 'mentioned', 'merely', 'message', 'messages', 'messaging', 'met', 'mexican', 'mexico', 'miami', 'michael', 'michael bloomberg', 'michael flynn', 'michelle', 'michigan', 'mid', 'middle', 'middle class', 'middle east', 'midst', 'migrants', 'mike', 'mike bloomberg', 'mike pence', 'mike pompeo', 'mild', 'miles', 'military', 'military aid', 'military bases', 'miller', 'million', 'million americans', 'million cable', 'million cable homes', 'million people', 'millions', 'millions americans', 'millions dollars', 'millions people', 'milwaukee', 'mind', 'minimum', 'minister', 'minn', 'minneapolis', 'minneapolis police', 'minnesota', 'minorities', 'minority', 'minority leader', 'minority leader chuck', 'minus', 'minus percentage', 'minus percentage points', 'minute', 'minutes', 'misconduct', 'misinformation', 'misleading', 'miss', 'missile', 'mission', 'mississippi', 'missouri', 'mistake', 'mistakes', 'mitch', 'mitch mcconnell', 'mitch mcconnell ky', 'mitigate', 'mitigation', 'mitt', 'mitt romney', 'mitt romney utah', 'mixed', 'mnuchin', 'mnuchin said', 'mob', 'mocked', 'model', 'moderate', 'modern', 'modly', 'moines', 'moment', 'moments', 'momentum', 'monday', 'monday night', 'money', 'montana', 'month', 'months', 'months ago', 'monument', 'monuments', 'moore', 'moral', 'moratorium', 'morning', 'morning joe', 'morris', 'mother', 'motion', 'motivated', 'mount', 'mount rushmore', 'moved', 'movement', 'moves', 'moving', 'mr', 'msnbc', 'mueller', 'mueller investigation', 'multiple', 'mulvaney', 'murder', 'murkowski', 'murphy', 'muslim', 'muslims', 'nadler', 'named', 'names', 'nancy', 'nancy calif', 'narrative', 'narrow', 'nascar', 'nation', 'national', 'national committee', 'national convention', 'national guard', 'national institute', 'national institute allergy', 'national intelligence', 'national security', 'national security adviser', 'national security council', 'nationally', 'nations', 'nationwide', 'nationwide protests', 'native', 'native american', 'natural', 'nature', 'navarro', 'navy', 'nazi', 'nba', 'nbc', 'nbc news', 'nd', 'near', 'nearby', 'nearly', 'nearly million', 'necessarily', 'necessary', 'neck', 'need', 'need help', 'needed', 'needs', 'negative', 'negotiate', 'negotiating', 'negotiations', 'neighborhood', 'neighborhoods', 'neil', 'net', 'network', 'network ch', 'network ch xfinity', 'networks', 'nevada', 'new', 'new cases', 'new coronavirus', 'new deal', 'new hampshire', 'new hampshire primary', 'new jersey', 'new mexico', 'new york', 'new york city', 'new york gov', 'new york mayor', 'new york state', 'new york times', 'new yorkers', 'newly', 'news', 'news america', 'news america newsroom', 'news conference', 'news fox', 'news fox friends', 'news host', 'news media', 'news outlet', 'news reported', 'news reports', 'news sunday', 'newsom', 'newspaper', 'newsroom', 'nfl', 'nice', 'night', 'nixon', 'noem', 'nominating', 'nomination', 'nominee', 'nominee joe', 'nominee joe biden', 'nominees', 'non', 'nonessential', 'nonprofit', 'normal', 'normally', 'north', 'north carolina', 'north korea', 'northern', 'notably', 'note', 'noted', 'notes', 'notice', 'noting', 'notion', 'nov', 'novel', 'novel coronavirus', 'november', 'november election', 'npr', 'nuclear', 'number', 'number cases', 'number people', 'numbers', 'numerous', 'nunes', 'nunited', 'nunited statesng', 'nunited statesng homes', 'nunited statess', 'ny', 'nypd', 'oath', 'obama', 'obama administration', 'objections', 'obstruction', 'obstruction congress', 'obtain', 'obtained', 'obvious', 'obviously', 'ocasio', 'ocasio cortez', 'occurred', 'october', 'odds', 'offensive', 'offer', 'offered', 'offering', 'offers', 'office', 'office said', 'officer', 'officers', 'offices', 'official', 'official said', 'official told', 'officially', 'officials', 'officials said', 'officials told', 'oh', 'ohio', 'oil', 'oil gas', 'ok', 'oklahoma', 'old', 'older', 'omar', 'ones', 'ongoing', 'online', 'op', 'op ed', 'open', 'opened', 'opening', 'openly', 'operate', 'operating', 'operation', 'operations', 'operatives', 'opinion', 'opinion piece', 'opinions', 'opponent', 'opponents', 'opportunities', 'opportunity', 'oppose', 'opposed', 'opposing', 'opposite', 'opposition', 'optimistic', 'optimum', 'optimum ch', 'optimum ch cox', 'option', 'options', 'order', 'ordered', 'ordering', 'orders', 'oregon', 'organization', 'organizations', 'organized', 'organizers', 'organizing', 'original', 'originally', 'origins', 'orlando', 'ought', 'ounited', 'ounited statesde', 'ounited stateslves', 'outbreak', 'outbreaks', 'outcome', 'outlet', 'outlets', 'outlined', 'outrage', 'outreach', 'outright', 'oval', 'oval office', 'overall', 'overcome', 'overseas', 'oversight', 'overwhelmed', 'overwhelming', 'overwhelmingly', 'owned', 'owner', 'owners', 'owns', 'pac', 'package', 'packed', 'pacs', 'page', 'pages', 'paid', 'paid leave', 'paid sick', 'paid sick leave', 'pain', 'pair', 'palestinian', 'palm', 'palm beach', 'pandemic', 'pandemic response', 'pandemic said', 'panel', 'panic', 'paper', 'pardon', 'parent', 'parenthood', 'parents', 'park', 'parks', 'parnas', 'parscale', 'participate', 'participated', 'participating', 'particular', 'particularly', 'parties', 'partisan', 'partner', 'partners', 'parts', 'party', 'party presidential', 'pass', 'passage', 'passed', 'passengers', 'passing', 'past', 'past week', 'past years', 'pastor', 'pat', 'path', 'patient', 'patients', 'patrick', 'patrol', 'pattern', 'paul', 'pay', 'paycheck', 'paycheck protection', 'paycheck protection program', 'paying', 'payment', 'payments', 'payroll', 'payroll tax', 'payroll tax cut', 'peace', 'peaceful', 'peaceful protesters', 'peak', 'penalty', 'pence', 'pence said', 'pending', 'pennsylvania', 'pentagon', 'people', 'people color', 'people country', 'people died', 'people going', 'people know', 'people need', 'people said', 'people united', 'people united states', 'people want', 'people work', 'percent', 'percentage', 'percentage points', 'perez', 'perfect', 'perform', 'performance', 'period', 'permanent', 'permit', 'person', 'personal', 'personal attorney', 'personal protective', 'personal protective equipment', 'personally', 'personnel', 'perspective', 'pete', 'pete buttigieg', 'peter', 'petition', 'phase', 'philadelphia', 'phoenix', 'phone', 'photo', 'photo op', 'photos', 'phrase', 'physical', 'physician', 'pick', 'picked', 'picture', 'piece', 'place', 'placed', 'places', 'plan', 'plane', 'planned', 'planned parenthood', 'planning', 'plans', 'plant', 'plants', 'platform', 'platforms', 'play', 'played', 'players', 'playing', 'plea', 'pleaded', 'pleaded guilty', 'pledged', 'plenty', 'plus', 'plus minus', 'plus minus percentage', 'podcast', 'point', 'pointed', 'pointing', 'points', 'police', 'police brutality', 'police chief', 'police custody', 'police department', 'police departments', 'police force', 'police killing', 'police killing george', 'police officer', 'police officers', 'police reform', 'police violence', 'policies', 'policing', 'policy', 'political', 'politically', 'politically motivated', 'politician', 'politicians', 'politico', 'politico reported', 'politics', 'poll', 'poll conducted', 'poll released', 'polling', 'polls', 'pompeo', 'poor', 'popular', 'population', 'populations', 'portion', 'portland', 'pose', 'posed', 'position', 'positions', 'positive', 'positive coronavirus', 'positive covid', 'positive virus', 'possibility', 'possible', 'possibly', 'post', 'post reported', 'postal', 'posted', 'postpone', 'postponed', 'posts', 'potential', 'potentially', 'poverty', 'powell', 'power', 'powerful', 'powers', 'ppe', 'ppp', 'practice', 'practices', 'praise', 'praised', 'praising', 'pre', 'precautions', 'precedent', 'precinct', 'predicted', 'prepare', 'prepared', 'preparing', 'presence', 'present', 'presented', 'presidency', 'president administration', 'president barack', 'president barack obama', 'president biden', 'president campaign', 'president clinton', 'president did', 'president does', 'president george', 'president george bush', 'president impeachment', 'president joe', 'president joe biden', 'president mike', 'president mike pence', 'president said', 'president told', 'president tweeted', 'president united', 'president united states', 'presidential', 'presidential bid', 'presidential campaign', 'presidential candidate', 'presidential candidates', 'presidential debate', 'presidential election', 'presidential nomination', 'presidential nominee', 'presidential nominee joe', 'presidential primary', 'presidential race', 'presidents', 'press', 'press briefing', 'press conference', 'press secretary', 'press secretary kayleigh', 'pressed', 'pressing', 'pressure', 'presumptive', 'presumptive democratic', 'presumptive democratic presidential', 'pretty', 'prevent', 'preventing', 'prevention', 'previous', 'previously', 'price', 'prices', 'primaries', 'primarily', 'primary', 'prime', 'prime minister', 'prior', 'priorities', 'priority', 'prison', 'prisoners', 'prisons', 'pritzker', 'privacy', 'private', 'privately', 'privilege', 'pro', 'probably', 'probe', 'problem', 'problems', 'procedures', 'proceedings', 'process', 'processing', 'produce', 'produced', 'product', 'production', 'products', 'professional', 'professionals', 'professor', 'profile', 'profit', 'program', 'programs', 'progress', 'progressive', 'progressives', 'project', 'projects', 'prominent', 'promise', 'promised', 'promises', 'promising', 'promote', 'promoted', 'promoting', 'prompted', 'prompting', 'proof', 'propaganda', 'proper', 'properly', 'property', 'proposal', 'proposals', 'proposed', 'prosecuted', 'prosecution', 'prosecutor', 'prosecutors', 'prospect', 'protect', 'protected', 'protecting', 'protection', 'protection program', 'protections', 'protective', 'protective equipment', 'protects', 'protest', 'protester', 'protesters', 'protesting', 'protests', 'protests police', 'proud', 'prove', 'proved', 'proven', 'provide', 'provided', 'providers', 'provides', 'providing', 'provision', 'provisions', 'proxy', 'public', 'public health', 'public health crisis', 'public health experts', 'public health officials', 'public safety', 'public schools', 'publication', 'publicly', 'published', 'puerto', 'puerto rico', 'pull', 'pulled', 'punited', 'purpose', 'purposes', 'push', 'pushed', 'pushing', 'putin', 'putting', 'qassem', 'qassem soleimani', 'qualified', 'qualified immunity', 'qualify', 'quality', 'quarantine', 'quarantined', 'quarter', 'quarters', 'queens', 'quick', 'quickly', 'quit', 'quite', 'qunited', 'qunited statesion', 'qunited statesioned', 'qunited statesioning', 'qunited statesions', 'quo', 'quoted', 'race', 'races', 'rachel', 'racial', 'racial justice', 'racism', 'racist', 'radical', 'radical left', 'radio', 'radio host', 'raise', 'raised', 'raised million', 'raises', 'raising', 'rallies', 'rally', 'rally tunited', 'rally tunited states', 'ramp', 'ran', 'rand', 'rand paul', 'range', 'ranking', 'ranks', 'rape', 'rapid', 'rapidly', 'rare', 'rarely', 'ratcliffe', 'rate', 'rates', 'rating', 'ratings', 'reach', 'reached', 'reaching', 'reaction', 'read', 'reade', 'reading', 'reads', 'ready', 'reagan', 'real', 'real estate', 'reality', 'really', 'reason', 'reasonable', 'reasons', 'rebuke', 'recalled', 'receive', 'received', 'receiving', 'recent', 'recent days', 'recent weeks', 'recent years', 'recently', 'recession', 'recipients', 'recognition', 'recognize', 'recognized', 'recommend', 'recommendation', 'recommendations', 'recommended', 'recommended feednetwork', 'record', 'recorded', 'recording', 'records', 'recover', 'recovery', 'red', 'redfield', 'reduce', 'reduced', 'reducing', 'reduction', 'reelection', 'reelection campaign', 'reference', 'referred', 'referring', 'reflect', 'reform', 'reforms', 'refusal', 'refuse', 'refused', 'refusing', 'regard', 'regarding', 'regardless', 'regime', 'region', 'register', 'registered', 'registered voters', 'registration', 'regular', 'regularly', 'regulation', 'regulations', 'reiterated', 'reject', 'rejected', 'related', 'relations', 'relationship', 'relationships', 'relatively', 'release', 'released', 'releasing', 'relevant', 'relief', 'relief package', 'religion', 'religious', 'reluctant', 'rely', 'remain', 'remained', 'remaining', 'remains', 'remarks', 'remember', 'remote', 'remotely', 'removal', 'remove', 'removed', 'removing', 'rent', 'reopen', 'reopen economy', 'reopening', 'rep', 'rep adam', 'rep alexandria', 'rep alexandria ocasio', 'repeat', 'repeated', 'repeatedly', 'replace', 'replied', 'report', 'reported', 'reportedly', 'reporter', 'reporters', 'reporting', 'reports', 'represent', 'representative', 'representatives', 'represented', 'representing', 'represents', 'reproductive', 'reps', 'republic', 'republican', 'republican controlled', 'republican lawmakers', 'republican national', 'republican national committee', 'republican party', 'republican president', 'republican said', 'republican said fox', 'republican sen', 'republican senators', 'republicans', 'republicans democrats', 'reputation', 'require', 'required', 'requirement', 'requirements', 'requires', 'requiring', 'requnited', 'requnited states', 'requnited states comment', 'requnited statesed', 'requnited statess', 'rescue', 'research', 'researchers', 'reserve', 'reserved', 'resident', 'residents', 'resign', 'resignation', 'resigned', 'resisted', 'resolution', 'resort', 'resources', 'respect', 'respiratory', 'respond', 'respond requnited', 'respond requnited states', 'responded', 'responding', 'response', 'response coronavirus', 'response coronavirus pandemic', 'response pandemic', 'responses', 'responsibility', 'responsible', 'rest', 'restaurant', 'restaurants', 'restore', 'restrict', 'restrictions', 'result', 'resulted', 'results', 'resume', 'retail', 'retaliation', 'retired', 'retirement', 'return', 'return work', 'returned', 'returning', 'returns', 'retweeted', 'reuters', 'rev', 'reveal', 'revealed', 'revenue', 'review', 'reviewed', 'revolution', 'rhetoric', 'rhode', 'rhode island', 'rice', 'rich', 'richard', 'richmond', 'rick', 'rico', 'rid', 'ridiculous', 'right', 'right thing', 'right vote', 'right wing', 'rights', 'rights reserved', 'riot', 'rioting', 'riots', 'ripped', 'rips', 'rise', 'rising', 'risk', 'risks', 'rival', 'rivals', 'rnc', 'road', 'robert', 'robert mueller', 'roberts', 'robust', 'roe', 'roger', 'roger stone', 'role', 'roll', 'rolled', 'romney', 'romney utah', 'ron', 'ron desantis', 'ronald', 'ronald reagan', 'room', 'rooms', 'roosevelt', 'rose', 'rose garden', 'roughly', 'round', 'roy', 'rubber', 'rubio', 'rudy', 'rudy giuliani', 'rule', 'rule law', 'ruled', 'rules', 'ruling', 'run', 'runited', 'runited statesa', 'runited statesa investigation', 'runited statesan', 'runited statesan interference', 'runited statesans', 'runner', 'running', 'running mate', 'runoff', 'rural', 'rush', 'rushmore', 'ryan', 'sad', 'safe', 'safely', 'safer', 'safety', 'said according', 'said adding', 'said administration', 'said asked', 'said biden', 'said campaign', 'said cnn', 'said democrats', 'said did', 'said does', 'said earlier', 'said fox', 'said fox news', 'said friday', 'said going', 'said interview', 'said just', 'said know', 'said monday', 'said need', 'said new', 'said news', 'said people', 'said president', 'said press', 'said referring', 'said said', 'said sen', 'said senate', 'said state', 'said statement', 'said sunday', 'said think', 'said thunited', 'said thunited statesay', 'said time', 'said tunited', 'said tunited statesay', 'said united', 'said united states', 'said want', 'said weare', 'said wednesday', 'said week', 'said white', 'said white house', 'sales', 'sample', 'san', 'san francisco', 'sanctions', 'sanders', 'sanders campaign', 'sanders said', 'sanders vt', 'sanitizer', 'saturday', 'saturday night', 'saudi', 'saudi arabia', 'save', 'save lives', 'saved', 'saving', 'saw', 'say', 'saying', 'says', 'scale', 'scalia', 'scalise', 'scandal', 'scarborough', 'scared', 'scenario', 'scene', 'schedule', 'scheduled', 'scheme', 'schiff', 'schiff calif', 'school', 'schools', 'schumer', 'schumer said', 'science', 'scientific', 'scientists', 'scope', 'scott', 'scrutiny', 'sea', 'sean', 'sean hannity', 'search', 'season', 'seat', 'seats', 'seattle', 'second', 'second term', 'secret', 'secret service', 'secretary', 'secretary kayleigh', 'secretary kayleigh mcenany', 'secretary mark', 'secretary mark esper', 'secretary state', 'secretary state mike', 'secretary steven', 'secretary steven mnuchin', 'section', 'sector', 'secure', 'security', 'security adviser', 'security adviser john', 'security adviser michael', 'security council', 'seeing', 'seek', 'seeking', 'seeks', 'seen', 'sees', 'segment', 'seized', 'sekulow', 'select', 'selected', 'self', 'self quarantine', 'sell', 'selling', 'sen', 'sen amy', 'sen amy klobuchar', 'sen bernie', 'sen bernie sanders', 'sen elizabeth', 'sen elizabeth warren', 'sen john', 'sen lindsey', 'sen lindsey graham', 'sen mitt', 'sen mitt romney', 'sen ted', 'sen tim', 'sen tim scott', 'senate', 'senate democrats', 'senate floor', 'senate impeachment', 'senate impeachment trial', 'senate judiciary', 'senate majority', 'senate majority leader', 'senate minority', 'senate minority leader', 'senate republicans', 'senate seat', 'senate trial', 'senate vote', 'senator', 'senators', 'send', 'sending', 'senior', 'senior adviser', 'seniors', 'sens', 'sense', 'sensitive', 'sent', 'sent letter', 'sentence', 'sentenced', 'sentences', 'sentencing', 'separate', 'sept', 'september', 'series', 'seriously', 'serve', 'served', 'serves', 'service', 'services', 'serving', 'session', 'sessions', 'set', 'setting', 'seven', 'severe', 'severity', 'sex', 'sexual', 'sexual assault', 'sexual harassment', 'sexually', 'shadow', 'shape', 'share', 'shared', 'sharing', 'sharp', 'shea', 'shelter', 'sheriff', 'shift', 'ship', 'shocking', 'shooting', 'shootings', 'short', 'short term', 'shortage', 'shortages', 'shortly', 'shot', 'showed', 'showing', 'shown', 'shows', 'shut', 'shutdown', 'shutdowns', 'shuttered', 'sick', 'sick leave', 'sides', 'sign', 'signature', 'signed', 'signed law', 'significant', 'significantly', 'signing', 'signs', 'silence', 'silent', 'similar', 'similarly', 'simple', 'simply', 'single', 'sit', 'site', 'sites', 'sitting', 'situation', 'situations', 'size', 'skeptical', 'slammed', 'slams', 'slavery', 'slightly', 'slow', 'slow spread', 'small', 'small business', 'small businesses', 'smaller', 'smart', 'smith', 'social', 'social distancing', 'social distancing guidelines', 'social distancing measures', 'social media', 'social security', 'socialist', 'society', 'sold', 'soldiers', 'soleimani', 'solution', 'solutions', 'solve', 'somebody', 'somewhat', 'son', 'son hunter', 'soon', 'sooner', 'sorry', 'sort', 'sought', 'sound', 'source', 'sources', 'south', 'south bend', 'south bend indiana', 'south carolina', 'south dakota', 'south korea', 'southern', 'space', 'spanish', 'sparked', 'speak', 'speaker', 'speaker nancy', 'speaker nancy calif', 'speaking', 'speaks', 'special', 'special counited', 'special counited statesl', 'specific', 'specifically', 'spectrum', 'spectrum verse', 'spectrum verse ch', 'speech', 'speed', 'spend', 'spending', 'spent', 'spent million', 'spicer', 'spike', 'split', 'spoke', 'spoke condition', 'spoke condition anonymity', 'spoken', 'spokesman', 'spokesperson', 'spokeswoman', 'sponsored', 'sports', 'spot', 'spots', 'spread', 'spread coronavirus', 'spread covid', 'spread virus', 'spreading', 'spring', 'square', 'st', 'st john', 'st lounited', 'staff', 'staff mark', 'staffer', 'staffers', 'stage', 'stake', 'stance', 'stand', 'standard', 'standards', 'standing', 'stands', 'star', 'stark', 'starr', 'start', 'started', 'starting', 'starts', 'state', 'state democratic', 'state department', 'state local', 'state local governments', 'state mike', 'state mike pompeo', 'state officials', 'state party', 'state sen', 'state union', 'state union address', 'stated', 'statement', 'statements', 'states', 'states ambassador', 'states attorney', 'states comment', 'states court', 'states department', 'states district', 'states district judge', 'states economy', 'states government', 'states house', 'states including', 'states intelligence', 'states like', 'states mexico', 'states military', 'states officials', 'states oklahoma', 'states president', 'states said', 'states senate', 'states supreme', 'states supreme court', 'states troops', 'statesa', 'statesa investigation', 'statesafter', 'statesan', 'statesan interference', 'statesana', 'statesance', 'statesances', 'statesand', 'statesans', 'statesantial', 'statesare', 'statesat', 'statesay', 'statesay afternoon', 'statesay morning', 'statesay night', 'statesay president', 'statesay said', 'statesbefore', 'statesd', 'statesde', 'statese', 'statesed', 'statesfor', 'stateshave', 'statesid', 'statesille', 'statesin', 'statesing', 'statesion', 'statesioned', 'statesioning', 'statesions', 'statesl', 'statesl robert', 'statesl robert mueller', 'stateslf', 'stateslor', 'stateslves', 'statesng', 'statesng homes', 'statesof', 'stateson', 'statesons', 'statesould', 'statess', 'statest', 'statesthat', 'statesthe', 'statesto', 'statewide', 'stating', 'station', 'stations', 'statue', 'statunited', 'status', 'statute', 'stay', 'stay home', 'stay home order', 'stay home orders', 'stayed', 'staying', 'steele', 'stefanik', 'stem', 'step', 'stephen', 'stepped', 'stepping', 'steps', 'steve', 'steven', 'steven mnuchin', 'steyer', 'stimulus', 'stimulus package', 'stock', 'stock market', 'stockpile', 'stocks', 'stone', 'stood', 'stop', 'stop frisk', 'stop spread', 'stopped', 'stopping', 'stops', 'store', 'stores', 'stories', 'story', 'straight', 'strategic', 'strategist', 'strategy', 'street', 'street journal', 'streets', 'strength', 'stressed', 'strict', 'strike', 'strikes', 'strong', 'stronger', 'strongly', 'struck', 'struggle', 'struggled', 'struggling', 'stuck', 'student', 'students', 'studies', 'study', 'stuff', 'style', 'subject', 'submitted', 'subpoena', 'subpoenas', 'suburban', 'success', 'successful', 'sudden', 'suddenlink', 'suddenlink ch', 'suddenlink ch cable', 'suddenlink ch mediacom', 'suddenly', 'sue', 'sued', 'suffered', 'suffering', 'suggest', 'suggested', 'suggesting', 'suggestion', 'suggests', 'suit', 'sullivan', 'summer', 'sunday', 'sunday morning', 'sunday night', 'sunited', 'sunited statesantial', 'super', 'super pac', 'super tunited', 'super tunited statesay', 'supplies', 'supply', 'supply chain', 'support', 'support president', 'supported', 'supporter', 'supporters', 'supporting', 'supports', 'supposed', 'suppression', 'supremacist', 'supreme', 'supreme court', 'supreme court justice', 'sure', 'surge', 'surprise', 'surprised', 'surrounding', 'surveillance', 'survey', 'surveyed', 'surveys', 'survive', 'survivors', 'susan', 'susan collins', 'susan collins maine', 'suspect', 'suspected', 'suspended', 'sweeping', 'swift', 'swing', 'symbol', 'symbols', 'symptoms', 'systemic', 'systemic racism', 'systems', 'systems click', 'table', 'tactics', 'taken', 'takes', 'taking', 'taliban', 'talk', 'talked', 'talking', 'talking points', 'talks', 'tally', 'tampering', 'tank', 'tapper', 'tara', 'tara reade', 'target', 'targeted', 'targeting', 'targets', 'tariffs', 'task', 'task force', 'tax', 'tax cunited', 'tax cut', 'tax returns', 'taxes', 'taxpayer', 'taxpayers', 'taylor', 'teacher', 'teachers', 'team', 'teams', 'tear', 'tear gas', 'tech', 'technical', 'technology', 'ted', 'ted cruz', 'tehran', 'television', 'tell', 'telling', 'tells', 'temporarily', 'temporary', 'tend', 'tennessee', 'tens', 'tens thousands', 'tensions', 'tenure', 'term', 'terms', 'terrible', 'territory', 'terrorism', 'terrorist', 'terrorists', 'test', 'tested', 'tested positive', 'tested positive coronavirus', 'tested positive covid', 'testified', 'testify', 'testimony', 'testing', 'tests', 'texas', 'text', 'th', 'thank', 'thanks', 'theories', 'theory', 'theyare', 'theyare going', 'theyhave', 'theywill', 'thing', 'things', 'think', 'think going', 'think people', 'thinking', 'thinks', 'thirds', 'thomas', 'thought', 'thousands', 'thousands people', 'threat', 'threaten', 'threatened', 'threatening', 'threats', 'threw', 'throw', 'thrown', 'thunited', 'thunited statesay', 'thunited statesay morning', 'thunited statesay night', 'ticket', 'tie', 'tied', 'ties', 'tim', 'tim scott', 'time', 'time said', 'timeline', 'times', 'times reported', 'timing', 'title', 'titled', 'today', 'todd', 'told', 'told associated', 'told associated press', 'told axios', 'told cnn', 'told fox', 'told fox news', 'told hill', 'told host', 'told msnbc', 'told nbc', 'told new', 'told new york', 'told politico', 'told reporters', 'told times', 'told tv', 'toll', 'tom', 'tom steyer', 'tomorrow', 'tone', 'tonight', 'tony', 'took', 'took office', 'took place', 'tool', 'tools', 'topic', 'total', 'totally', 'touch', 'tough', 'tour', 'touted', 'touting', 'town', 'town hall', 'tracing', 'track', 'trade', 'trade agreement', 'trade deal', 'traditional', 'traffic', 'trail', 'trailing', 'training', 'transgender', 'transition', 'transmission', 'transparency', 'transportation', 'travel', 'travel ban', 'traveled', 'traveling', 'treasury', 'treasury department', 'treasury secretary', 'treasury secretary steven', 'treat', 'treated', 'treating', 'treatment', 'treatments', 'tremendous', 'trend', 'trial', 'trial president', 'tribal', 'tribe', 'tribes', 'tried', 'trillion', 'trip', 'troops', 'trouble', 'true', 'truly', 'trust', 'trusted', 'truth', 'try', 'trying', 'tuberville', 'tucker', 'tucker carlson', 'tunited', 'tunited states', 'tunited states oklahoma', 'tunited statesay', 'tunited statesay night', 'tunited statesay said', 'turn', 'turned', 'turning', 'turnout', 'turns', 'tv', 'tv carried', 'tv carried million', 'tweet', 'tweeted', 'tweeting', 'tweets', 'twice', 'twitter', 'twitter users', 'type', 'typically', 'ukraine', 'ukrainian', 'ultimately', 'unable', 'unacceptable', 'uncertainty', 'unclear', 'unconstitutional', 'underlying', 'undermine', 'understand', 'understanding', 'undocumented', 'unemployed', 'unemployment', 'unemployment benefits', 'unemployment insurance', 'unemployment rate', 'unfair', 'unfortunately', 'union', 'union address', 'unions', 'unique', 'unit', 'unite', 'united', 'united states', 'united states ambassador', 'united states attorney', 'united states court', 'united states district', 'united states economy', 'united states government', 'united states intelligence', 'united states mexico', 'united states military', 'united states officials', 'united states president', 'united states senate', 'united states supreme', 'united states troops', 'united statest', 'units', 'unity', 'universal', 'universities', 'university', 'unless', 'unlike', 'unlikely', 'unnamed', 'unnecessary', 'unprecedented', 'unrest', 'unusual', 'unveiled', 'upcoming', 'update', 'updated', 'updates', 'upper', 'urban', 'urge', 'urged', 'urgency', 'urgent', 'urges', 'urging', 'usa', 'use', 'use force', 'used', 'users', 'uses', 'using', 'usual', 'usually', 'utah', 'va', 'vaccine', 'vaccines', 'valley', 'value', 'valunited', 'valunited states', 'van', 'variety', 'various', 'vast', 'vast majority', 'vegas', 'vehicle', 'venezuela', 'ventilators', 'vermont', 'vermont senator', 'verse', 'verse ch', 'verse ch fios', 'version', 'veteran', 'veterans', 'veto', 'vice', 'vice president', 'vice president biden', 'vice president joe', 'vice president mike', 'vice presidential', 'victim', 'victims', 'victory', 'video', 'videos', 'view', 'viewed', 'viewers', 'views', 'vindman', 'violate', 'violated', 'violating', 'violation', 'violations', 'violence', 'violent', 'viral', 'virginia', 'virtual', 'virtually', 'virus', 'virus spread', 'visas', 'vision', 'visit', 'visited', 'visiting', 'visitors', 'vital', 'vladimir', 'vladimir putin', 'vocal', 'voice', 'voices', 'volunteer', 'volunteers', 'vote', 'vote mail', 'voted', 'voter', 'voter fraud', 'voters', 'votes', 'voting', 'voting mail', 'voting rights', 'vowed', 'vp', 'vt', 'vulnerable', 'wage', 'wages', 'wait', 'waiting', 'wake', 'walk', 'walked', 'walker', 'walking', 'wall', 'wall street', 'wall street journal', 'wallace', 'walsh', 'want', 'want make', 'wanted', 'wants', 'war', 'war ii', 'warn', 'warned', 'warning', 'warnings', 'warns', 'warren', 'warren campaign', 'warren mass', 'warren said', 'wars', 'wash', 'washington', 'washington ap', 'washington post', 'washington post reported', 'washington state', 'watch', 'watchdog', 'watched', 'watching', 'water', 'wave', 'way', 'ways', 'weak', 'wealth', 'wealthy', 'weapon', 'weapons', 'wear', 'wear mask', 'wear masks', 'weare', 'weare doing', 'weare going', 'weare seeing', 'wearing', 'wearing mask', 'wearing masks', 'weather', 'website', 'wednesday', 'wednesday morning', 'wednesday night', 'week', 'week said', 'weekend', 'weekly', 'weeks', 'weeks ago', 'wehave', 'wehave got', 'wehave seen', 'weigh', 'weinstein', 'welcome', 'went', 'west', 'west virginia', 'western', 'wewill', 'wheeler', 'whistleblower', 'white', 'white house', 'white house chief', 'white house coronavirus', 'white house correspondent', 'white house counited', 'white house official', 'white house officials', 'white house press', 'white police', 'white police officer', 'white supremacist', 'whitmer', 'wide', 'widely', 'widespread', 'wife', 'william', 'william barr', 'williams', 'willing', 'willingness', 'wilson', 'win', 'wind', 'wing', 'winner', 'winning', 'wins', 'wisconsin', 'wish', 'withdraw', 'withheld', 'withhold', 'withholding', 'witness', 'witnesses', 'wolf', 'wolf said', 'woman', 'women', 'won', 'word', 'words', 'wore', 'work', 'work said', 'worked', 'worker', 'workers', 'workforce', 'working', 'working class', 'workplace', 'works', 'world', 'world health', 'world health organization', 'world war', 'world war ii', 'worldwide', 'worried', 'worries', 'worry', 'worse', 'worship', 'worst', 'worth', 'wray', 'write', 'writer', 'writing', 'written', 'wrong', 'wrongdoing', 'wrote', 'wrote letter', 'wrote twitter', 'wuhan', 'wyden', 'xfinity', 'xfinity ch', 'xfinity ch spectrum', 'xi', 'xi jinping', 'yang', 'yeah', 'year', 'year old', 'years', 'years ago', 'yes', 'yesterday', 'yoho', 'york', 'york city', 'york city mayor', 'york gov', 'york gov andrew', 'york mayor', 'york state', 'york times', 'york times reported', 'yorkers', 'youare', 'youare going', 'yougov', 'youhave', 'youhave got', 'young', 'young people', 'younger', 'younited', 'younited statese', 'younited stateslf', 'younited statesould', 'youth', 'youtube', 'youwill', 'yovanovitch', 'zelensky', 'zero', 'zone']\n"
     ]
    }
   ],
   "source": [
    "# See all feature words - not too useful\n",
    "# But can be...if you know some top ones from LinearSVC's.\n",
    "feature_names = count_vect.get_feature_names()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display an accuracy report for all pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_files = ['h_and_n_onegram_sr_nol_nopr.pkl', 'h_and_n_onegram_sr_l_nopr.pkl',\n",
    "                'h_and_n_bigram_sr_nol_nopr.pkl', 'h_and_n_bigram_sr_l_nopr.pkl',\n",
    "                'h_and_n_trigram_sr_nol_nopr.pkl', 'h_and_n_trigram_sr_l_nopr.pkl',\n",
    "                'hs_and_nr_onegram_sr_nol_nopr.pkl', 'hs_and_nr_onegram_sr_l_nopr.pkl',\n",
    "                'hs_and_nr_bigram_sr_nol_nopr.pkl', 'hs_and_nr_bigram_sr_l_nopr.pkl',\n",
    "                'hs_and_nr_trigram_sr_nol_nopr.pkl', 'hs_and_nr_trigram_sr_l_nopr.pkl',\n",
    "                'hsr_and_nrw_onegram_sr_nol_nopr.pkl', 'hsr_and_nrw_onegram_sr_l_nopr.pkl',\n",
    "                'hsr_and_nrw_bigram_sr_nol_nopr.pkl', 'hsr_and_nrw_bigram_sr_l_nopr.pkl',\n",
    "                'hsr_and_nrw_trigram_sr_nol_nopr.pkl', 'hsr_and_nrw_trigram_sr_l_nopr.pkl',\n",
    "                'h_and_n_onegram_sr_nol_pr.pkl', 'h_and_n_onegram_sr_l_pr.pkl',\n",
    "                'h_and_n_bigram_sr_nol_pr.pkl', 'h_and_n_bigram_sr_l_pr.pkl',\n",
    "                'h_and_n_trigram_sr_nol_pr.pkl', 'h_and_n_trigram_sr_l_pr.pkl',\n",
    "                'hs_and_nr_onegram_sr_nol_pr.pkl', 'hs_and_nr_onegram_sr_l_pr.pkl',\n",
    "                'hs_and_nr_bigram_sr_nol_pr.pkl', 'hs_and_nr_bigram_sr_l_pr.pkl',\n",
    "                'hs_and_nr_trigram_sr_nol_pr.pkl', 'hs_and_nr_trigram_sr_l_pr.pkl',\n",
    "                'hsr_and_nrw_onegram_sr_nol_pr.pkl', 'hsr_and_nrw_onegram_sr_l_pr.pkl',\n",
    "                'hsr_and_nrw_bigram_sr_nol_pr.pkl', 'hsr_and_nrw_bigram_sr_l_pr.pkl',\n",
    "                'hsr_and_nrw_trigram_sr_nol_pr.pkl', 'hsr_and_nrw_trigram_sr_l_pr.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Sources - Testing against\n",
      "                                   Huffington  Slate  Salon  TalkingPointsMemo  Alternet  Rawstory  Overall (excluding TalkingPointsMemo)\n",
      "h_and_n_onegram_sr_nol_nopr.pkl    43.030      65.138 57.826 28.006             48.332    56.083    54.082\n",
      "h_and_n_onegram_sr_l_nopr.pkl      93.262      73.183 81.178 53.586             63.940    74.806    77.274\n",
      "h_and_n_bigram_sr_nol_nopr.pkl     96.050      79.758 86.613 58.439             67.169    79.465    81.811\n",
      "h_and_n_bigram_sr_l_nopr.pkl       93.216      77.422 84.600 55.591             66.738    76.273    79.650\n",
      "h_and_n_trigram_sr_nol_nopr.pkl    95.678      78.374 85.455 59.546             67.277    79.551    81.267\n",
      "h_and_n_trigram_sr_l_nopr.pkl      41.380      58.824 62.104 28.903             44.349    47.196    50.770\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl  85.362      56.920 99.396 49.262             60.388    72.649    74.943\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl    84.108      59.862 99.497 49.789             62.110    71.786    75.472\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl   86.106      59.602 99.547 45.992             59.526    75.065    75.969\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl     83.573      59.256 99.396 49.473             60.280    76.790    75.859\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl  85.177      57.007 99.597 44.989             56.082    70.492    73.671\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl    81.413      55.536 99.295 45.517             56.728    72.304    73.055\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl74.373      55.709 97.433 44.146             64.370    98.533    78.084\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl  71.608      54.498 97.484 41.139             62.002    98.706    76.860\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl 70.725      52.509 97.484 38.924             61.141    98.792    76.130\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl   71.120      49.481 97.735 40.981             61.464    98.533    75.667\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl74.210      54.498 98.390 41.930             65.124    98.533    78.151\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl  69.586      48.702 97.484 38.660             58.881    98.792    74.689\n",
      "h_and_n_onegram_sr_nol_pr.pkl      93.216      74.654 86.563 62.500             69.537    87.489    82.292\n",
      "h_and_n_onegram_sr_l_pr.pkl        89.847      70.069 82.335 58.017             64.801    83.607    78.132\n",
      "h_and_n_bigram_sr_nol_pr.pkl       96.213      80.882 89.079 68.460             73.843    90.854    86.174\n",
      "h_and_n_bigram_sr_l_pr.pkl         91.078      74.308 85.355 60.654             70.506    87.144    81.678\n",
      "h_and_n_trigram_sr_nol_pr.pkl      95.469      79.498 88.123 66.139             71.690    89.560    84.868\n",
      "h_and_n_trigram_sr_l_pr.pkl        91.032      74.048 78.963 60.074             67.277    84.297    79.123\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl    87.454      60.986 99.094 58.703             65.770    84.814    79.624\n",
      "hs_and_nr_onegram_sr_l_pr.pkl      80.460      56.574 98.138 52.637             58.235    80.328    74.747\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl     85.362      59.516 99.245 52.215             61.895    81.104    77.424\n",
      "hs_and_nr_bigram_sr_l_pr.pkl       84.689      59.516 99.144 55.802             63.940    83.607    78.179\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl    87.779      62.976 99.396 56.276             64.586    83.607    79.669\n",
      "hs_and_nr_trigram_sr_l_pr.pkl      85.781      61.332 99.547 57.753             63.186    84.987    78.967\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl  74.721      58.651 96.477 49.789             63.186    97.670    78.141\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl    63.894      50.260 93.961 40.928             53.283    95.427    71.365\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl   73.954      57.093 98.188 47.890             63.186    98.533    78.191\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl     13.151      25.952 70.408 8.597             22.282    24.763    31.311\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl  72.723      56.661 97.232 46.466             59.957    98.016    76.918\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl    8.550      19.464 38.148 5.116             12.487    14.236    18.577\n"
     ]
    }
   ],
   "source": [
    "# Test on all sources\n",
    "confidence_level = .53\n",
    "\n",
    "left_source = [\"Huffington\", \"Slate\", \"Salon\", \"TalkingPointsMemo\", \"Alternet\", \"Rawstory\"]\n",
    "#left_test = zip(left_source, pickle_files) \n",
    "left_overall_acc = []\n",
    "huff_acc = []\n",
    "salon_acc = []\n",
    "rawstory_acc = []\n",
    "\n",
    "print(\"Left Sources - Testing against\")\n",
    "print(\"%45s  %s  %s  %s  %s  %s  %s\" %(left_source[0], left_source[1], left_source[2], left_source[3], \n",
    "                                       left_source[4], left_source[5], \"Overall (excluding TalkingPointsMemo)\"))\n",
    "for pickle in pickle_files:\n",
    "    accuracys = []\n",
    "    total_acc = 0\n",
    "    for source in left_source:\n",
    "        text_clf = joblib.load(pickle)\n",
    "        test_data = pd.read_excel(\"csvs/\" + source + \".xlsx\", \n",
    "                      names=[\"date\", \"article\"])\n",
    "\n",
    "        # Check for null values\n",
    "        test_data['date'].fillna(\"\", inplace=True)\n",
    "        test_data['article'].fillna(\"\", inplace=True)\n",
    "\n",
    "        for x in range(test_data.shape[0]):\n",
    "            if len(test_data['article'][x]) < length_req:\n",
    "                test_data.drop(x, inplace=True)\n",
    "\n",
    "        # Only keep the unique article rows and their values\n",
    "        test_data.drop_duplicates(\"article\", keep='first', inplace=True)\n",
    "\n",
    "        test_data['date'] = test_data['date'].str.replace(',', '')\n",
    "\n",
    "        # This is a more clean and thorough url decoding function for decoding any character string...\n",
    "        test_data['article'] = test_data['article'].astype(str).apply(lambda x: html.unescape(x))\n",
    "\n",
    "        #test_data.head()\n",
    "\n",
    "        # Check for null values\n",
    "        #print(test_data.isnull().sum(axis=0))\n",
    "\n",
    "        # If there were null values, the below will replace them. - Sometimes dates are missing when transferred over.\n",
    "        test_data['date'].fillna(\"\", inplace=True)\n",
    "        #print(test_data.isnull().sum(axis=0))\n",
    "\n",
    "        ## All pole entries - Have a look at how confident the model is on each individual entry.\n",
    "        ## Confidence level - State how many pole entries you want to see that the model has classified above a specific confidence level\n",
    "        confident_entries = 0\n",
    "\n",
    "        test_data['pole'] = 0  # Make a column 'pole', assign a value of 0 to indicate left articles\n",
    "\n",
    "        # predicted - should be an array of the predictions of the model in order that the articles come in.\n",
    "        predicted = text_clf.predict(test_data['article'])\n",
    "        class_probabilities = text_clf.predict_proba(test_data['article'])\n",
    "\n",
    "        for x in range(test_data.shape[0]):\n",
    "            if class_probabilities[x][0] > confidence_level:\n",
    "                confident_entries += 1\n",
    "\n",
    "        # Accuracy\n",
    "        acc = metrics.accuracy_score(test_data.pole, predicted)\n",
    "        #print(\"Entire dataset accuracy:\", acc, end='\\n\\n')\n",
    "        \n",
    "        if (source == \"Huffington\"):\n",
    "            huff_acc.append(acc*100)\n",
    "        elif (source == \"Salon\"):\n",
    "            salon_acc.append(acc*100)\n",
    "        elif (source == \"Rawstory\"):\n",
    "            rawstory_acc.append(acc*100)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        accuracys.append(acc*100)\n",
    "    \n",
    "    # Display pickle file used with accuracys\n",
    "    for x in range(len(accuracys)):\n",
    "        if x == 3:\n",
    "            pass\n",
    "        else:\n",
    "            total_acc += accuracys[x]\n",
    "    total_acc = total_acc / (len(accuracys) - 1)\n",
    "    left_overall_acc.append(total_acc)\n",
    "    print(\"%-35s%.3f      %.3f %.3f %5.3f         %10.3f    %.3f    %.3f\" % (pickle, accuracys[0], accuracys[1], \n",
    "                                                                             accuracys[2], accuracys[3], accuracys[4], \n",
    "                                                                             accuracys[5], total_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right Sources - Testing against\n",
      "                                      NewsMax  NationalReview  Redstate  WashingtonExaminer  TheBulwark  Overall (Excluding Bulwark)\n",
      "h_and_n_onegram_sr_nol_nopr.pkl       81.747   56.228          45.713    72.737              23.702      64.106\n",
      "h_and_n_onegram_sr_l_nopr.pkl         93.074   48.962          43.297    63.683              30.796      62.254\n",
      "h_and_n_bigram_sr_nol_nopr.pkl        93.218   45.675          37.944    63.886              25.260      60.181\n",
      "h_and_n_bigram_sr_l_nopr.pkl          92.860   47.405          40.786    66.836              30.450      61.972\n",
      "h_and_n_trigram_sr_nol_nopr.pkl       93.146   43.945          40.171    64.191              29.239      60.363\n",
      "h_and_n_trigram_sr_l_nopr.pkl         85.075   55.190          54.192    73.550              33.045      67.002\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl     87.205   68.166          99.005    68.057              55.536      80.608\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl       86.382   63.668          98.816    65.107              50.692      78.493\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl      88.099   66.955          99.005    69.685              53.287      80.936\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl        86.757   66.782          98.626    68.769              51.211      80.234\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl     88.386   71.626          99.242    70.397              52.941      82.413\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl       88.314   71.453          99.053    72.431              55.882      82.813\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl   85.755   76.125          95.168    98.372              51.903      88.855\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl     86.596   73.875          95.405    97.457              52.076      88.333\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl    87.563   79.239          96.684    98.779              55.190      90.566\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl      85.666   77.336          96.447    99.288              56.747      89.684\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl   86.382   76.298          95.973    98.576              56.747      89.307\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl     86.883   79.066          96.589    98.881              62.457      90.355\n",
      "h_and_n_onegram_sr_nol_pr.pkl         88.368   42.907          41.497    64.903              28.720      59.419\n",
      "h_and_n_onegram_sr_l_pr.pkl           88.547   44.291          42.729    65.209              30.623      60.194\n",
      "h_and_n_bigram_sr_nol_pr.pkl          84.717   37.024          34.439    53.204              24.740      52.346\n",
      "h_and_n_bigram_sr_l_pr.pkl            86.471   44.637          39.792    64.802              28.893      58.925\n",
      "h_and_n_trigram_sr_nol_pr.pkl         85.648   38.235          37.755    57.274              25.087      54.728\n",
      "h_and_n_trigram_sr_l_pr.pkl           87.223   43.426          41.118    64.090              31.142      58.964\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl       79.474   56.747          95.879    59.919              48.962      73.005\n",
      "hs_and_nr_onegram_sr_l_pr.pkl         82.838   63.668          96.068    64.496              54.152      76.768\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl        81.890   62.803          96.305    65.819              52.422      76.704\n",
      "hs_and_nr_bigram_sr_l_pr.pkl          79.563   58.131          94.079    61.343              49.308      73.279\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl       80.029   59.516          94.884    61.750              45.329      74.044\n",
      "hs_and_nr_trigram_sr_l_pr.pkl         77.792   58.131          93.510    58.901              48.097      72.084\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl     82.838   68.685          89.484    95.219              49.135      84.056\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl       87.777   74.048          91.047    97.050              59.862      87.481\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl      83.447   67.301          89.152    94.914              49.827      83.703\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl        95.866   81.142          84.368    90.336              64.187      87.928\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl     84.592   68.512          89.815    96.236              53.979      84.789\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl       97.691   87.716          92.468    95.219              74.740      93.274\n"
     ]
    }
   ],
   "source": [
    "# Now test on the right sources\n",
    "right_source = [\"NewsMax\", \"NationalReview\", \"Redstate\", \"WashingtonExaminer\", \"TheBulwark\"]\n",
    "right_overall_acc = []\n",
    "newsmax_acc = []\n",
    "redstate_acc = []\n",
    "washington_acc = []\n",
    "\n",
    "print(\"Right Sources - Testing against\")\n",
    "print(\"%45s  %s  %s  %s  %s  %s\" %(right_source[0], right_source[1], right_source[2], \n",
    "                                    right_source[3], right_source[4], \"Overall (Excluding Bulwark)\"))\n",
    "for pickle in pickle_files:\n",
    "    accuracys = []\n",
    "    total_acc = 0\n",
    "    for source in right_source:\n",
    "        text_clf = joblib.load(pickle)\n",
    "        test_data = pd.read_excel(\"csvs/\" + source + \".xlsx\", \n",
    "                      names=[\"date\", \"article\"])\n",
    "\n",
    "        # If there were null values, the below will replace them. - Sometimes dates are missing when transferred over.\n",
    "        test_data['date'].fillna(\"\", inplace=True)\n",
    "        test_data['article'].fillna(\"\", inplace=True)\n",
    "        #print(test_data.isnull().sum(axis=0))\n",
    "\n",
    "        for x in range(test_data.shape[0]):\n",
    "            if len(test_data['article'][x]) < length_req:\n",
    "                test_data.drop(x, inplace=True)\n",
    "\n",
    "        # Only keep the unique article rows and their values\n",
    "        test_data.drop_duplicates(\"article\", keep='first', inplace=True)\n",
    "\n",
    "        test_data['date'] = test_data['date'].str.replace(',', '')\n",
    "\n",
    "        # This is a more clean and thorough url decoding function for decoding any character string...\n",
    "        test_data['article'] = test_data['article'].astype(str).apply(lambda x: html.unescape(x))\n",
    "\n",
    "        ## All pole entries - Have a look at how confident the model is on each individual entry.\n",
    "        ## Confidence level - State how many pole entries you want to see that the model has classified above a specific confidence level\n",
    "        confident_entries = 0\n",
    "\n",
    "        test_data['pole'] = 1  # Make a column 'pole', assign a value of 1 to indicate right articles\n",
    "\n",
    "        # predicted - should be an array of the predictions of the model in order that the articles come in.\n",
    "        predicted = text_clf.predict(test_data['article'])\n",
    "        class_probabilities = text_clf.predict_proba(test_data['article'])\n",
    "\n",
    "        for x in range(test_data.shape[0]):\n",
    "            if class_probabilities[x][0] > confidence_level:\n",
    "                confident_entries += 1\n",
    "                \n",
    "        # Accuracy\n",
    "        acc = metrics.accuracy_score(test_data.pole, predicted)\n",
    "        #print(\"Entire dataset accuracy:\", acc, end='\\n\\n')\n",
    "    \n",
    "        if (source == \"NewsMax\"):\n",
    "            newsmax_acc.append(acc*100)\n",
    "        elif (source == \"Redstate\"):\n",
    "            redstate_acc.append(acc*100)\n",
    "        elif (source == \"WashingtonExaminer\"):\n",
    "            washington_acc.append(acc*100)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        accuracys.append(acc*100)\n",
    "    \n",
    "    # Display pickle file used with accuracys\n",
    "    for x in range(len(accuracys) - 1):\n",
    "        total_acc += accuracys[x]\n",
    "    total_acc = total_acc / (len(accuracys) - 1)\n",
    "    right_overall_acc.append(total_acc)\n",
    "    print(\"%-35s   %.3f   %.3f          %.3f    %5.3f          %10.3f      %.3f\" % (pickle, accuracys[0], accuracys[1], accuracys[2],\n",
    "                                                                     accuracys[3], accuracys[4], total_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell starts accuracy across every source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire dataset accuracy per pickle file:\n",
      "h_and_n_onegram_sr_nol_nopr.pkl     59.094\n",
      "h_and_n_onegram_sr_l_nopr.pkl       69.764\n",
      "h_and_n_bigram_sr_nol_nopr.pkl      70.996\n",
      "h_and_n_bigram_sr_l_nopr.pkl        70.811\n",
      "h_and_n_trigram_sr_nol_nopr.pkl     70.815\n",
      "h_and_n_trigram_sr_l_nopr.pkl       58.886\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl   77.776\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl     76.983\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl    78.453\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl      78.046\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl   78.042\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl     77.934\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl 83.469\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl   82.596\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl  83.348\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl    82.675\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl 83.729\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl   82.522\n",
      "h_and_n_onegram_sr_nol_pr.pkl       70.855\n",
      "h_and_n_onegram_sr_l_pr.pkl         69.163\n",
      "h_and_n_bigram_sr_nol_pr.pkl        69.260\n",
      "h_and_n_bigram_sr_l_pr.pkl          70.302\n",
      "h_and_n_trigram_sr_nol_pr.pkl       69.798\n",
      "h_and_n_trigram_sr_l_pr.pkl         69.044\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl     76.314\n",
      "hs_and_nr_onegram_sr_l_pr.pkl       75.757\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl      77.064\n",
      "hs_and_nr_bigram_sr_l_pr.pkl        75.729\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl     76.857\n",
      "hs_and_nr_trigram_sr_l_pr.pkl       75.525\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl   81.099\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl     79.423\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl    80.947\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl      59.619\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl   80.853\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl     55.925\n"
     ]
    }
   ],
   "source": [
    "final_results = []\n",
    "print(\"Entire dataset accuracy per pickle file:\")\n",
    "for x in range(len(left_overall_acc)):\n",
    "    total_accuracy = 0\n",
    "    total_accuracy += (left_overall_acc[x] + right_overall_acc[x]) / 2\n",
    "    print(\"%-35s %.3f\" % (pickle_files[x], total_accuracy))\n",
    "    final_results.append([pickle_files[x], total_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted final results:\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl 83.729\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl 83.469\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl  83.348\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl    82.675\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl   82.596\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl   82.522\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl   81.099\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl    80.947\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl   80.853\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl     79.423\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl    78.453\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl      78.046\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl   78.042\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl     77.934\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl   77.776\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl      77.064\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl     76.983\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl     76.857\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl     76.314\n",
      "hs_and_nr_onegram_sr_l_pr.pkl       75.757\n",
      "hs_and_nr_bigram_sr_l_pr.pkl        75.729\n",
      "hs_and_nr_trigram_sr_l_pr.pkl       75.525\n",
      "h_and_n_bigram_sr_nol_nopr.pkl      70.996\n",
      "h_and_n_onegram_sr_nol_pr.pkl       70.855\n",
      "h_and_n_trigram_sr_nol_nopr.pkl     70.815\n",
      "h_and_n_bigram_sr_l_nopr.pkl        70.811\n",
      "h_and_n_bigram_sr_l_pr.pkl          70.302\n",
      "h_and_n_trigram_sr_nol_pr.pkl       69.798\n",
      "h_and_n_onegram_sr_l_nopr.pkl       69.764\n",
      "h_and_n_bigram_sr_nol_pr.pkl        69.260\n",
      "h_and_n_onegram_sr_l_pr.pkl         69.163\n",
      "h_and_n_trigram_sr_l_pr.pkl         69.044\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl      59.619\n",
      "h_and_n_onegram_sr_nol_nopr.pkl     59.094\n",
      "h_and_n_trigram_sr_l_nopr.pkl       58.886\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl     55.925\n"
     ]
    }
   ],
   "source": [
    "print(\"Sorted final results:\")\n",
    "final_results.sort(key=lambda x: x[1], reverse=True)\n",
    "for x in range(len(final_results)):\n",
    "    print(\"%-35s %.3f\" % (final_results[x][0], final_results[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         onegram  bigram   trigram\n",
      "         83.469   83.348   83.729  \n",
      "         82.596   82.675   82.522  \n",
      "         81.099   80.947   80.853  \n",
      "         79.423   78.453   78.042  \n",
      "         77.776   78.046   77.934  \n",
      "         76.983   77.064   76.857  \n",
      "         76.314   75.729   75.525  \n",
      "         75.757   70.996   70.815  \n",
      "         70.855   70.811   69.798  \n",
      "         69.764   70.302   69.044  \n",
      "         69.163   69.260   58.886  \n",
      "         59.094   59.619   55.925  \n",
      "\n",
      "Overall: 75.191   74.771   73.327  \n"
     ]
    }
   ],
   "source": [
    "# Final results according to n-grams.\n",
    "pickle_results = {}\n",
    "onegram = []\n",
    "bigram = []\n",
    "trigram = []\n",
    "onegram_overall = 0\n",
    "bigram_overall = 0\n",
    "trigram_overall = 0\n",
    "total_one = 0\n",
    "total_bi = 0\n",
    "total_tri = 0\n",
    "\n",
    "# Separate the results according to gram usage.\n",
    "for x in range(len(final_results)):\n",
    "    if \"onegram\" in final_results[x][0]:\n",
    "        onegram.append(final_results[x][1])\n",
    "    elif \"bigram\" in final_results[x][0]:\n",
    "        bigram.append(final_results[x][1])\n",
    "    else:\n",
    "        trigram.append(final_results[x][1])\n",
    "\n",
    "# Dictionary\n",
    "#pickle_results[\"onegram\"] = onegram\n",
    "#pickle_results[\"bigram\"] = bigram\n",
    "#pickle_results[\"trigram\"] = trigram\n",
    "#for key, value in pickle_results.items():\n",
    "#    if key == \"onegram\":\n",
    "#        print(value)\n",
    "\n",
    "for x in range(len(onegram)):\n",
    "    total_one += onegram[x]\n",
    "    total_bi += bigram[x]\n",
    "    total_tri += trigram[x]\n",
    "    \n",
    "onegram_overall = total_one / len(onegram)\n",
    "bigram_overall = total_bi / len(bigram)\n",
    "trigram_overall = total_tri / len(trigram)\n",
    "\n",
    "print(\"         onegram  bigram   trigram\")\n",
    "for x in range(len(onegram)):\n",
    "    print(\"         %.3f   %.3f   %.3f  \" % (onegram[x], bigram[x], trigram[x]))\n",
    "print(\"\\nOverall: %.3f   %.3f   %.3f  \" % (onegram_overall, bigram_overall, trigram_overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the difference between grams\n",
    "pickle_files_by_gram = ['h_and_n_onegram_sr_nol_nopr.pkl', 'h_and_n_bigram_sr_nol_nopr.pkl', 'h_and_n_trigram_sr_nol_nopr.pkl',\n",
    "                        'h_and_n_onegram_sr_l_nopr.pkl', 'h_and_n_bigram_sr_l_nopr.pkl', 'h_and_n_trigram_sr_l_nopr.pkl',\n",
    "                        'hs_and_nr_onegram_sr_nol_nopr.pkl', 'hs_and_nr_bigram_sr_nol_nopr.pkl', 'hs_and_nr_trigram_sr_nol_nopr.pkl', \n",
    "                        'hs_and_nr_onegram_sr_l_nopr.pkl', 'hs_and_nr_bigram_sr_l_nopr.pkl', 'hs_and_nr_trigram_sr_l_nopr.pkl',\n",
    "                        'hsr_and_nrw_onegram_sr_nol_nopr.pkl', 'hsr_and_nrw_bigram_sr_nol_nopr.pkl', 'hsr_and_nrw_trigram_sr_nol_nopr.pkl', \n",
    "                        'hsr_and_nrw_onegram_sr_l_nopr.pkl', 'hsr_and_nrw_bigram_sr_l_nopr.pkl', 'hsr_and_nrw_trigram_sr_l_nopr.pkl',\n",
    "                        'h_and_n_onegram_sr_nol_pr.pkl', 'h_and_n_bigram_sr_nol_pr.pkl', 'h_and_n_trigram_sr_nol_pr.pkl', \n",
    "                        'h_and_n_onegram_sr_l_pr.pkl', 'h_and_n_bigram_sr_l_pr.pkl', 'h_and_n_trigram_sr_l_pr.pkl',\n",
    "                        'hs_and_nr_onegram_sr_nol_pr.pkl', 'hs_and_nr_bigram_sr_nol_pr.pkl', 'hs_and_nr_trigram_sr_nol_pr.pkl', \n",
    "                        'hs_and_nr_onegram_sr_l_pr.pkl', 'hs_and_nr_bigram_sr_l_pr.pkl', 'hs_and_nr_trigram_sr_l_pr.pkl',\n",
    "                        'hsr_and_nrw_onegram_sr_nol_pr.pkl', 'hsr_and_nrw_bigram_sr_nol_pr.pkl', 'hsr_and_nrw_trigram_sr_nol_pr.pkl', \n",
    "                        'hsr_and_nrw_onegram_sr_l_pr.pkl', 'hsr_and_nrw_bigram_sr_l_pr.pkl', 'hsr_and_nrw_trigram_sr_l_pr.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h_and_n_onegram_sr_nol_nopr.pkl       59.094\n",
      "h_and_n_bigram_sr_nol_nopr.pkl        70.996\n",
      "h_and_n_trigram_sr_nol_nopr.pkl       70.815\n",
      "\n",
      "h_and_n_onegram_sr_l_nopr.pkl         69.764\n",
      "h_and_n_bigram_sr_l_nopr.pkl          70.811\n",
      "h_and_n_trigram_sr_l_nopr.pkl         58.886\n",
      "\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl     77.776\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl      78.453\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl     78.042\n",
      "\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl       76.983\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl        78.046\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl       77.934\n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl   83.469\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl    83.348\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl   83.729\n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl     82.596\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl      82.675\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl     82.522\n",
      "\n",
      "h_and_n_onegram_sr_nol_pr.pkl         70.855\n",
      "h_and_n_bigram_sr_nol_pr.pkl          69.260\n",
      "h_and_n_trigram_sr_nol_pr.pkl         69.798\n",
      "\n",
      "h_and_n_onegram_sr_l_pr.pkl           69.163\n",
      "h_and_n_bigram_sr_l_pr.pkl            70.302\n",
      "h_and_n_trigram_sr_l_pr.pkl           69.044\n",
      "\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl       76.314\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl        77.064\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl       76.857\n",
      "\n",
      "hs_and_nr_onegram_sr_l_pr.pkl         75.757\n",
      "hs_and_nr_bigram_sr_l_pr.pkl          75.729\n",
      "hs_and_nr_trigram_sr_l_pr.pkl         75.525\n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl     81.099\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl      80.947\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl     80.853\n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl       79.423\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl        59.619\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl       55.925\n",
      "\n",
      "Onegram > Bigram and Trigram accuracy:   4 / 12 times\n",
      "Bigram > Onegram and Bigram accuracy:    7 / 12 times\n",
      "Trigram > Onegram and Trigram accuracy:  1 / 12 times\n"
     ]
    }
   ],
   "source": [
    "grams = []\n",
    "one = 0\n",
    "bi = 0\n",
    "tri = 0\n",
    "one_count = 0\n",
    "bi_count = 0 \n",
    "tri_count = 0\n",
    "multiples = [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35]\n",
    "\n",
    "for param in pickle_files_by_gram:\n",
    "    for x in range(len(final_results)):\n",
    "        if param in final_results[x][0]:\n",
    "            grams.append((final_results[x][0], final_results[x][1]))\n",
    "\n",
    "for x in range(len(grams)):\n",
    "    # Mutiples of 3...\n",
    "    if (x % 3 == 0):\n",
    "        if x == 0:\n",
    "            one = grams[x][1]\n",
    "        if one == 0:\n",
    "            one = grams[x][1]\n",
    "        print(\"\\n%-35s   %.3f\" % (grams[x][0], grams[x][1]))\n",
    "    else:\n",
    "        if bi == 0:\n",
    "            bi = grams[x][1]\n",
    "        if x in multiples:\n",
    "            tri = grams[x][1]\n",
    "        print(\"%-35s   %.3f\" % (grams[x][0], grams[x][1]))\n",
    "    \n",
    "    # As x reaches a number in the list, clear variables and loop again\n",
    "    if x in multiples:\n",
    "        if (one > bi and one > tri):\n",
    "            one_count += 1\n",
    "        elif (bi > one and bi > tri):\n",
    "            bi_count += 1\n",
    "        else:\n",
    "            tri_count += 1\n",
    "        one = 0\n",
    "        bi = 0\n",
    "        tri = 0\n",
    "\n",
    "print(\"\\nOnegram > Bigram and Trigram accuracy:   %i / %i times\" % (one_count, len(pickle_files_by_gram) / 3))\n",
    "print(\"Bigram > Onegram and Bigram accuracy:    %i / %i times\" % (bi_count, len(pickle_files_by_gram) / 3))\n",
    "print(\"Trigram > Onegram and Trigram accuracy:  %i / %i times\" % (tri_count, len(pickle_files_by_gram) / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the difference between lemmatization vs no lemmatization\n",
    "pickle_files_by_lemmatization = ['h_and_n_onegram_sr_nol_nopr.pkl', 'h_and_n_onegram_sr_l_nopr.pkl',\n",
    "                'h_and_n_bigram_sr_nol_nopr.pkl', 'h_and_n_bigram_sr_l_nopr.pkl',\n",
    "                'h_and_n_trigram_sr_nol_nopr.pkl', 'h_and_n_trigram_sr_l_nopr.pkl',\n",
    "                'hs_and_nr_onegram_sr_nol_nopr.pkl', 'hs_and_nr_onegram_sr_l_nopr.pkl',\n",
    "                'hs_and_nr_bigram_sr_nol_nopr.pkl', 'hs_and_nr_bigram_sr_l_nopr.pkl',\n",
    "                'hs_and_nr_trigram_sr_nol_nopr.pkl', 'hs_and_nr_trigram_sr_l_nopr.pkl',\n",
    "                'hsr_and_nrw_onegram_sr_nol_nopr.pkl', 'hsr_and_nrw_onegram_sr_l_nopr.pkl',\n",
    "                'hsr_and_nrw_bigram_sr_nol_nopr.pkl', 'hsr_and_nrw_bigram_sr_l_nopr.pkl',\n",
    "                'hsr_and_nrw_trigram_sr_nol_nopr.pkl', 'hsr_and_nrw_trigram_sr_l_nopr.pkl',\n",
    "                'h_and_n_onegram_sr_nol_pr.pkl', 'h_and_n_onegram_sr_l_pr.pkl',\n",
    "                'h_and_n_bigram_sr_nol_pr.pkl', 'h_and_n_bigram_sr_l_pr.pkl',\n",
    "                'h_and_n_trigram_sr_nol_pr.pkl', 'h_and_n_trigram_sr_l_pr.pkl',\n",
    "                'hs_and_nr_onegram_sr_nol_pr.pkl', 'hs_and_nr_onegram_sr_l_pr.pkl',\n",
    "                'hs_and_nr_bigram_sr_nol_pr.pkl', 'hs_and_nr_bigram_sr_l_pr.pkl',\n",
    "                'hs_and_nr_trigram_sr_nol_pr.pkl', 'hs_and_nr_trigram_sr_l_pr.pkl',\n",
    "                'hsr_and_nrw_onegram_sr_nol_pr.pkl', 'hsr_and_nrw_onegram_sr_l_pr.pkl',\n",
    "                'hsr_and_nrw_bigram_sr_nol_pr.pkl', 'hsr_and_nrw_bigram_sr_l_pr.pkl',\n",
    "                'hsr_and_nrw_trigram_sr_nol_pr.pkl', 'hsr_and_nrw_trigram_sr_l_pr.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h_and_n_onegram_sr_nol_nopr.pkl       59.094\n",
      "h_and_n_onegram_sr_l_nopr.pkl         69.764\n",
      "\n",
      "h_and_n_bigram_sr_nol_nopr.pkl        70.996\n",
      "h_and_n_bigram_sr_l_nopr.pkl          70.811\n",
      "\n",
      "h_and_n_trigram_sr_nol_nopr.pkl       70.815\n",
      "h_and_n_trigram_sr_l_nopr.pkl         58.886\n",
      "\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl     77.776\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl       76.983\n",
      "\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl      78.453\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl        78.046\n",
      "\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl     78.042\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl       77.934\n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl   83.469\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl     82.596\n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl    83.348\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl      82.675\n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl   83.729\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl     82.522\n",
      "\n",
      "h_and_n_onegram_sr_nol_pr.pkl         70.855\n",
      "h_and_n_onegram_sr_l_pr.pkl           69.163\n",
      "\n",
      "h_and_n_bigram_sr_nol_pr.pkl          69.260\n",
      "h_and_n_bigram_sr_l_pr.pkl            70.302\n",
      "\n",
      "h_and_n_trigram_sr_nol_pr.pkl         69.798\n",
      "h_and_n_trigram_sr_l_pr.pkl           69.044\n",
      "\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl       76.314\n",
      "hs_and_nr_onegram_sr_l_pr.pkl         75.757\n",
      "\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl        77.064\n",
      "hs_and_nr_bigram_sr_l_pr.pkl          75.729\n",
      "\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl       76.857\n",
      "hs_and_nr_trigram_sr_l_pr.pkl         75.525\n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl     81.099\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl       79.423\n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl      80.947\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl        59.619\n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl     80.853\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl       55.925\n",
      "\n",
      "No Lemmatization > Lemmatization accuracy:   16 / 18 times\n",
      "Lemmatization > No Lemmatization accuracy:   2 / 18 times\n"
     ]
    }
   ],
   "source": [
    "lemmas = []\n",
    "one = 0\n",
    "bi = 0\n",
    "one_count = 0\n",
    "bi_count = 0 \n",
    "multiples = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35]\n",
    "\n",
    "for param in pickle_files_by_lemmatization:\n",
    "    for x in range(len(final_results)):\n",
    "        if param in final_results[x][0]:\n",
    "            lemmas.append((final_results[x][0], final_results[x][1]))\n",
    "\n",
    "for x in range(len(lemmas)):\n",
    "    # Mutiples of 2...\n",
    "    if (x % 2 == 0):\n",
    "        if x == 0:\n",
    "            one = lemmas[x][1]\n",
    "        if one == 0:\n",
    "            one = lemmas[x][1]\n",
    "        print(\"\\n%-35s   %.3f\" % (lemmas[x][0], lemmas[x][1]))\n",
    "    else:\n",
    "        if bi == 0:\n",
    "            bi = lemmas[x][1]\n",
    "        print(\"%-35s   %.3f\" % (lemmas[x][0], lemmas[x][1]))\n",
    "        \n",
    "    # As x reaches a number in the list, clear variables and loop again\n",
    "    if x in multiples:\n",
    "        if (one > bi):\n",
    "            one_count += 1\n",
    "        else:\n",
    "            bi_count += 1\n",
    "        one = 0\n",
    "        bi = 0\n",
    "\n",
    "print(\"\\nNo Lemmatization > Lemmatization accuracy:   %i / %i times\" % (one_count, len(pickle_files_by_lemmatization) / 2))\n",
    "print(\"Lemmatization > No Lemmatization accuracy:   %i / %i times\" % (bi_count, len(pickle_files_by_lemmatization) / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the difference between lemmatization vs no lemmatization\n",
    "pickle_files_by_propernoun = ['h_and_n_onegram_sr_nol_nopr.pkl', 'h_and_n_onegram_sr_nol_pr.pkl',\n",
    "                              'h_and_n_onegram_sr_l_nopr.pkl', 'h_and_n_onegram_sr_l_pr.pkl',\n",
    "                              'h_and_n_bigram_sr_nol_nopr.pkl', 'h_and_n_bigram_sr_nol_pr.pkl',\n",
    "                              'h_and_n_bigram_sr_l_nopr.pkl', 'h_and_n_bigram_sr_l_pr.pkl',\n",
    "                              'h_and_n_trigram_sr_nol_nopr.pkl', 'h_and_n_trigram_sr_nol_pr.pkl',\n",
    "                              'h_and_n_trigram_sr_l_nopr.pkl', 'h_and_n_trigram_sr_l_pr.pkl',\n",
    "                              'hs_and_nr_onegram_sr_nol_nopr.pkl', 'hs_and_nr_onegram_sr_nol_pr.pkl',\n",
    "                              'hs_and_nr_onegram_sr_l_nopr.pkl', 'hs_and_nr_onegram_sr_l_pr.pkl',\n",
    "                              'hs_and_nr_bigram_sr_nol_nopr.pkl', 'hs_and_nr_bigram_sr_nol_pr.pkl',\n",
    "                              'hs_and_nr_bigram_sr_l_nopr.pkl', 'hs_and_nr_bigram_sr_l_pr.pkl',\n",
    "                              'hs_and_nr_trigram_sr_nol_nopr.pkl', 'hs_and_nr_trigram_sr_nol_pr.pkl',\n",
    "                              'hs_and_nr_trigram_sr_l_nopr.pkl', 'hs_and_nr_trigram_sr_l_pr.pkl',\n",
    "                              'hsr_and_nrw_onegram_sr_nol_nopr.pkl', 'hsr_and_nrw_onegram_sr_nol_pr.pkl',\n",
    "                              'hsr_and_nrw_onegram_sr_l_nopr.pkl', 'hsr_and_nrw_onegram_sr_l_pr.pkl',\n",
    "                              'hsr_and_nrw_bigram_sr_nol_nopr.pkl', 'hsr_and_nrw_bigram_sr_nol_pr.pkl',\n",
    "                              'hsr_and_nrw_bigram_sr_l_nopr.pkl', 'hsr_and_nrw_bigram_sr_l_pr.pkl',\n",
    "                              'hsr_and_nrw_trigram_sr_nol_nopr.pkl', 'hsr_and_nrw_trigram_sr_nol_pr.pkl',\n",
    "                              'hsr_and_nrw_trigram_sr_l_nopr.pkl', 'hsr_and_nrw_trigram_sr_l_pr.pkl']         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h_and_n_onegram_sr_nol_nopr.pkl       59.094\n",
      "h_and_n_onegram_sr_nol_pr.pkl         70.855\n",
      "\n",
      "h_and_n_onegram_sr_l_nopr.pkl         69.764\n",
      "h_and_n_onegram_sr_l_pr.pkl           69.163\n",
      "\n",
      "h_and_n_bigram_sr_nol_nopr.pkl        70.996\n",
      "h_and_n_bigram_sr_nol_pr.pkl          69.260\n",
      "\n",
      "h_and_n_bigram_sr_l_nopr.pkl          70.811\n",
      "h_and_n_bigram_sr_l_pr.pkl            70.302\n",
      "\n",
      "h_and_n_trigram_sr_nol_nopr.pkl       70.815\n",
      "h_and_n_trigram_sr_nol_pr.pkl         69.798\n",
      "\n",
      "h_and_n_trigram_sr_l_nopr.pkl         58.886\n",
      "h_and_n_trigram_sr_l_pr.pkl           69.044\n",
      "\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl     77.776\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl       76.314\n",
      "\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl       76.983\n",
      "hs_and_nr_onegram_sr_l_pr.pkl         75.757\n",
      "\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl      78.453\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl        77.064\n",
      "\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl        78.046\n",
      "hs_and_nr_bigram_sr_l_pr.pkl          75.729\n",
      "\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl     78.042\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl       76.857\n",
      "\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl       77.934\n",
      "hs_and_nr_trigram_sr_l_pr.pkl         75.525\n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl   83.469\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl     81.099\n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl     82.596\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl       79.423\n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl    83.348\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl      80.947\n",
      "\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl      82.675\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl        59.619\n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl   83.729\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl     80.853\n",
      "\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl     82.522\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl       55.925\n",
      "\n",
      "No Proper Noun Removal > Proper Noun Removal accuracy:   16 / 18 times\n",
      "Proper Noun Removal > No Proper Noun Removal accuracy:   2 / 18 times\n"
     ]
    }
   ],
   "source": [
    "propernoun = []\n",
    "one = 0\n",
    "bi = 0\n",
    "one_count = 0\n",
    "bi_count = 0 \n",
    "multiples = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35]\n",
    "\n",
    "for param in pickle_files_by_propernoun:\n",
    "    for x in range(len(final_results)):\n",
    "        if param in final_results[x][0]:\n",
    "            propernoun.append((final_results[x][0], final_results[x][1]))\n",
    "\n",
    "for x in range(len(propernoun)):\n",
    "    # Mutiples of 2...\n",
    "    if (x % 2 == 0):\n",
    "        if x == 0:\n",
    "            one = propernoun[x][1]\n",
    "        if one == 0:\n",
    "            one = propernoun[x][1]\n",
    "        print(\"\\n%-35s   %.3f\" % (propernoun[x][0], propernoun[x][1]))\n",
    "    else:\n",
    "        if bi == 0:\n",
    "            bi = propernoun[x][1]\n",
    "        print(\"%-35s   %.3f\" % (propernoun[x][0], propernoun[x][1]))\n",
    "        \n",
    "    # As x reaches a number in the list, clear variables and loop again\n",
    "    if x in multiples:\n",
    "        if (one > bi):\n",
    "            one_count += 1\n",
    "        else:\n",
    "            bi_count += 1\n",
    "        one = 0\n",
    "        bi = 0\n",
    "\n",
    "print(\"\\nNo Proper Noun Removal > Proper Noun Removal accuracy:   %i / %i times\" % (one_count, len(pickle_files_by_propernoun) / 2))\n",
    "print(\"Proper Noun Removal > No Proper Noun Removal accuracy:   %i / %i times\" % (bi_count, len(pickle_files_by_propernoun) / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl: 83.729\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl: 83.469\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl: 83.348\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl: 82.675\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl: 82.596\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl: 82.522\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl: 81.099\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl: 80.947\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl: 80.853\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl: 79.423\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl: 78.453\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl: 78.046\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl: 78.042\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hs_and_nr_trigram_sr_l_nopr.pkl: 77.934\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl: 77.776\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl: 77.064\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl: 76.983\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl: 76.857\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl: 76.314\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_l_pr.pkl: 75.757\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_l_pr.pkl: 75.729\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_l_pr.pkl: 75.525\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_nol_nopr.pkl: 70.996\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_onegram_sr_nol_pr.pkl: 70.855\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_nol_nopr.pkl: 70.815\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_l_nopr.pkl: 70.811\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_l_pr.pkl: 70.302\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_and_n_trigram_sr_nol_pr.pkl: 69.798\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_onegram_sr_l_nopr.pkl: 69.764\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_nol_pr.pkl: 69.260\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_onegram_sr_l_pr.pkl: 69.163\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_l_pr.pkl: 69.044\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl: 59.619\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_onegram_sr_nol_nopr.pkl: 59.094\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_l_nopr.pkl: 58.886\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl: 55.925\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the classifier each model uses.\n",
    "for x in range(len(final_results)):\n",
    "    text_clf = joblib.load(final_results[x][0])\n",
    "    \n",
    "    try:\n",
    "        clf = text_clf.named_steps['clf']\n",
    "        count_vect = text_clf.named_steps['vect']\n",
    "        print(\"%s: %.3f\" % (final_results[x][0], final_results[x][1]))\n",
    "        print(clf)\n",
    "        print(\"%s \\n\" % (count_vect))\n",
    "    except:\n",
    "        clf = text_clf.named_steps['eclf']\n",
    "        count_vect = text_clf.named_steps['vect']\n",
    "        print(\"%s: %.3f\" % (final_results[x][0], final_results[x][1]))\n",
    "        print(clf)\n",
    "        print(\"%s \\n\" % (count_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VotingClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-306bcd7cd8bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eclf'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vect'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtop_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshow_most_informative_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_vect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-99d7750c3529>\u001b[0m in \u001b[0;36mshow_most_informative_features\u001b[1;34m(vectorizer, clf, n)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtop_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Get all the feature names that CountVectorizer() is using\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mcoefs_with_fns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Put all feature names with their weights. Sort.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoefs_with_fns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoefs_with_fns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\\t%-15s\\t\\t\\t%-15s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VotingClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "# RandomForest and VotingClassifier - Have no coef_ function\n",
    "# Therefore, you cannot see the frequency of each feature those classifiers are using\n",
    "# You'll need to use the 2nd best classifier to see the frequency of the top features.\n",
    "# Use LinearSVC classifier in Top_Features.ipynb (SGD - 3rd best)\n",
    "text_clf = joblib.load(final_results[1][0])\n",
    "clf = text_clf.named_steps['eclf']\n",
    "count_vect = text_clf.named_steps['vect']\n",
    "top_features = show_most_informative_features(count_vect, clf, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left? 0.099581 Right? 0.900419\n",
      "['2020-08-03, Tucker Carlson: Equality under law is slipping away The Jeffrey Epstein case paints a picture of a justice system in which the rich and well-connected can do virtually whatever they want. With America’s institutions under relentless attack — and in some cases crumbling — it’s worth thinking through what we’d like to save from the ashes. When the revolution finally ends. what do we hope to have left? In other words, what are our best traditions? There are a lot of them. At the very top of the list is equality under the law. Equality is the most basic of all American ideals. It’s the very first principle articulated in the Declaration of Independence. It’s why the founders broke with England. In America, all citizens would be subject to the same rules: The same standards. The same penalties. Rich or poor. Black or White. All of us are equal under the law. TUCKER CARLSON: POLITICAL VIOLENCE IS AN ATTACK ON AMERICA ITSELF That’s the promise. It’s easier to explain than to achieve, of course. But we’ve tried hard. We should be proud of that. Yet some in power are no longer trying. Equality, the thing we’ve fought to keep for centuries, is slipping away. The Jeffrey Epstein case is the latest example. Thursday night, dozens of unsealed court documents from the Epstein case emerged online. They paint a picture of a justice system in which the rich and well-connected can do virtually whatever they want. In one sworn deposition, a woman called Virginia Giuffre claims that Epstein and others — including Prince Andrew of England and attorney Alan Dershowitz —\\xa0sexually abused her as a minor. Giuffre says an FBI agent responded that the agency didn’t plan to do anything about it. Epstein’s case, he allegedly said, wasn’t “going anywhere” because of “the chain of command.” The documents then describe what appears to be a remarkable abuse of power. Epstein’s accusers claim that Alan Dershowitz and Prince Andrew helped Epstein beat federal charges for sex crimes in 2008. They also allege that Dershowitz crafted the immunity agreement so that he himself also wouldn’t face criminal prosecution. Dershowitz has denied all of this. \\xa0  Alex Acosta will not be joining us. He’s the former Labor Department secretary who at the time was a federal prosecutor. Acosta is the one who agreed to the Epstein deal. When asked why he let a sex abuser skate, Acosta reportedly said: “I was told Epstein ‘belonged to intelligence’ and to leave it alone.” What does that mean exactly? What intelligence service did Jeffrey Epstein work for? Why did our government allow him to sexually abuse little girls? We deserve answers to those questions. But so far, no one is providing them. Epstein can’t tell us. He’s dead. The press, strangely, doesn’t seem very interested in finding the answers. It’s possible that’s because Epstein was close to a remarkable number of prominent Democratic politicians.   The new documents suggest that former president Bill Clinton visited Epstein’s private island in the Caribbean with two young girls. Giuffre claims Epstein made her have sex with former New Mexico Gov. Bill Richardson, former U.S. Sen. George Mitchell, and a famous Massachusetts Institute of Technology scientist. Is any of this true? We don’t know. Ghislaine Maxwell probably does. She’s in custody and set to go on trial. Will she explain what happened and why? Doesn’t look promising. Her lawyers fought the release of these documents.']\n"
     ]
    }
   ],
   "source": [
    "# Test against some new Fox News - Opinion articles\n",
    "input_arr = [\"2020-08-03, Deroy Murdock: Orwellian Democrats claim Portland's violence = peace Andy McCarthy provides insight to AG Bill testifying before the House judiciary committee NEW YORK — Denial is not just a river in Egypt, as the ancient joke goes. It’s the Democrat/media/Left’s comprehensive response to the nationwide riots that rage on, seven weeks after the funeral of police-brutality victim George Floyd. His memory has been kidnapped by the most violent insurrectionists America has seen since 1968. “Portland, Oregon is not out of control,” U.S. Rep. Earl Blumenauer, D-Ore., reassured his House colleagues on July 21. What about the projectiles, fires and explosions that have rocked that city since early June? If you have seen such things on TV, they must have been special effects. Or perhaps you have been experiencing acid flashbacks — a not-so-subtle reminder of your colorful youth. “Do you disavow the violence from Antifa that’s happening in Portland right now?” journalist Austen Fleccas asked Rep. Jerrold Nadler, D-N.Y., on Sunday. “That’s a myth that’s being spread only in Washington, D.C.,” Nadler replied. PORTLAND POLICE OVERTIME PAY EXCEEDED $5.3M SINCE PROTESTS STARTED Two days later, Nadler further explained Portland’s mythical instability. He excoriated William Barr at a hearing/verbal abuse session that the House Judiciary Committee arranged for the attorney general. Clearly enraged by the Trump administration’s surge of federal officers sent to Portland to combat the carnage that is not happening there, Nadler said, “The president wants footage for his campaign ads, and you appear to be serving it up to him as ordered.” The Judiciary chairman added: “Now you are projecting fear and violence nationwide in pursuit of obvious political objectives. Shame on you, Mr. Barr. Shame on you.” “The playbook is to create the impression that there is violence, that he must send in federal troops,” Rep. Zoe Lofgren, D-Calif., said at Tuesday’s inquisition. “And that is how he [President Trump] hopes to win the election.” “People are showing up because the troops are there,” added Lofgren. “Most of them are non-violent.” Of course, as a 13-term congresswoman, Lofgren knows the difference between troops, who are not there, and federal civilian officers, who are confronting the impression of violence.   “Most of the protests have been peaceful, Mr. Barr. You know that,” scolded Rep. Debbie Mucarsel-Powell, D-Fla. “In most of these cities, the protests had begun to wind down before you marched in and confronted the protesters.” Gov. Kate Brown, D-Ore., wrote Wednesday via Twitter that federal agents “have acted as an occupying force & brought violence.” Brown’s words echoed those of Portland's Democratic Mayor Ted Wheeler. As he wrote via Twitter on July 14, “my biggest immediate concern is the violence federal officers brought to our streets in recent days… We do not need or want their help.” But on July 3, Wheeler blamed others for his city’s chaos. “I remain deeply concerned, however, by groups who continue to perpetrate violence and vandalism on our streets,” Wheeler wrote via Twitter, a day before federal Homeland Security officers reached Portland. “This has been going on for more than a month now,” Wheeler continued, suggesting that Stumptown’s unrest began around June 3, four weeks before ”Trump’s troops” arrived and, as Leftist liars contend, magically transformed peaceful protesters into rioters, all so Trump would have scary villains to attack en route to reelection. Wheeler added, in consecutive Twitter messages on July 3: “Groups continue to target the Justice Center, threatening the safety of hundreds of inmates and employees inside...They continue to hurt small businesses owned by people of color, instill fear in communities of color, and start fires in buildings with people inside, in one specific case, even bolting emergency doors so that they could not escape.” Unlike House Democrats, who mainly told Barr to shut up, Republicans introduced a radical reform: They asked Barr questions and let him answer. “As far as the weapons you mentioned, let me get this straight,” said Rep. Steve Chabot, R-Ohio. He listed “rifles, explosives, knives, saws, sledgehammers, Tasers, slingshots, rocks, bricks, lasers. Have I missed anything?” “You have missed some things, but that’s a good list,” Barr replied. “They have these powerful slingshots with ball bearings that they shoot. They have used pellet guns, we believe. We have found those projectiles have penetrated Marshals to the bone. They use the lasers to blind the Marshals. They do start fires. They start fires, if they can get the fire inside or through the windows. And they start fires along the outside of the courthouse. When the Marshals come out to try to deal with the fires, they are assaulted.” “Federal courthouses are under attack,” Barr reminded the Committee on the Judiciary, no less. Seemingly exasperated with oblivious, or totally dishonest, Democrats, Barr wondered: “Since when is it OK to try to burn down a federal court? If someone went down the street to the Prettyman Court here, that beautiful courthouse we have right at the bottom of the Hill, and started breaking windows and firing industrial grade fireworks, and, to start a fire, throw kerosene balloons and start fires in the court. Is that OK? Is that OK now?”        Federal officers advance on demonstrators during a Black Lives Matter protest at the Mark O. Hatfield US Courthouse, July 25, 2020.<strong> </strong> (AP Photo/Marcio Jose Sanchez)        According to DHS, “Violent anarchists targeted surveillance cameras around the Hatfield Courthouse, rendering them inoperable.” Also on July 19-20, “The U.S. Marshals Service reported communications jamming last night, which may have caused significant problems with their radio communications.” The Associated Press’ Michael Balsamo embedded himself inside the Mark Hatfield U.S. Courthouse. His dispatches, via Twitter, are chilling. “I watched as injured officers were hauled inside. In one case, the commercial firework came over so fast the officer didn’t have time to respond. It burned through his sleeves & he had bloody gashes on both forearms. Another had a concussion from being hit in the head w/ a mortar.” Balsamo added: “The lights inside the courthouse have to be turned off for safety & the light from high-powered lasers bounced across the lobby almost all night. The fear is palpable. Three officers were struck in the last few weeks & still haven’t regained their vision.”   DHS’s deployment to Portland is not ritual chest-beating. It’s not toxic masculinity. It’s the law. According to 40 U.S. Code § 1315, the Secretary of Homeland Security “shall protect the buildings, grounds, and property that are owned, occupied, or secured by the Federal Government.” Acting Secretary Chad Wolf would break federal law if he left the courthouse undefended and let Antifa & Co. burn it to the ground. Of course, if the Hatfield Courthouse went up in smoke, the same people decrying President Trump’s supposed fascism would erupt like Klaxons: “Why didn’t he stop this? He was asleep at the switch! Wake up, Mr. President!”   Attorney General Barr had every reason to be perplexed by the radical Democrats’ institutional indifference toward these relentless onslaughts against federal personnel and what they are guarding: a palace of justice. “What makes me concerned for the country is this is the first time in my memory that the leaders of one of our great two political parties, the Democratic Party, are not coming out and condemning mob violence and the attack on federal courts,” Barr said. “Why can’t we just say violence against federal courts has to stop? Could we hear something like that?” \",       \n",
    "\"2020-08-03, Andrew McCarthy: Court rejection of Boston bomber’s death sentence seems based on hostility to death penalty  A couple of weeks back, when the Justice Department endeavored to restart executions of inmates sentenced to death by juries for unspeakable murders only to have federal judges (appointed by President Obama, in the main) throw up roadblocks, I repeated an observation I’ve made several times over the years. “Because much of the bench is hostile to the death penalty, judges are wont to fashion reasons not to impose it, some of which have nothing ostensibly to do with the death penalty and make prosecution of other types of criminals more difficult,” I wrote. We saw this again Friday. A federal appeals court in Boston threw out the death sentence of Dzhokar Tsarnaev, who brutally killed three people and injured more than 260 others when he and his late brother, Tamerlan, bombed the 2013 Boston Marathon. FEDERAL APPEALS COURT VACATES BOSTON MARATHON BOMBER DZHOKHAR TSARNAEV'S DEATH SENTENCE The three-judge panel consisted of two Obama appointees, Judges O. Rogeriee Thompson (who wrote the nearly 200-page opinion) and William J. Kayatta Jr., who formed the majority. A Reagan appointee, Juan R. Torruella, concurred in the result and much of the reasoning. Because of the decision’s girth, more time will be needed to study it. The upshot of the ruling, however, is that the trial judge failed to ensure that the Boston jury could be fair and impartial in light of all the prejudicial pretrial publicity. There is a strong suggestion that the trial judge should have granted a change of venue. This seems utterly unpersuasive to me. To start with, if there is grave doubt that Tsarnaev got a fair trial under the circumstances, then why does the court leave the bulk of his convictions undisturbed? A terrorist who bombs Boston is not going to be viewed with detachment and objectivity if he is instead tried in Philadelphia or Houston. The court does reverse three firearms convictions, but on technical legal grounds not because of jury prejudice. (Aside: Most Americans will be puzzled by the technical legal rationale, which leads to the court’s conclusion that Tsarnaev, a terrorist, was not engaged in a “crime of violence” while he was carrying a firearm.) Yet, the court takes pains to assure everyone that Tsarnaev “will remain confined to prison for the rest of his life.” The only remaining questions are whether the government will choose to re-try the death penalty phase of the case, and whether a new jury will unanimously vote for a capital sentence in a proceeding that the reviewing court — someday, years from now — decides passes its evolving standards of fairness. Why? If the jury was inflamed by unfair prejudice from the start, then why does the court believe Tsarnaev’s convictions should stand? That a minimum sentence of life imprisonment must stand? That only the death penalty must be revisited? I prosecuted terrorists in a courthouse that was a few blocks away from the World Trade Center that they had conspired to bomb. Our courtroom was similarly within easy walking distance of the FBI’s New York field office and the Holland Tunnel, which were also on the jihadists’ target list. To be sure, it was not a death penalty case, but the same issues of prejudicial pretrial publicity existed. The suggestion that it is not possible for a defendant to get a fair trial in the city he has terrorized is far-fetched.   The court intimates that the challenge of insulating a jury from publicity is more daunting today than it has ever been because the Internet and social media make publicity ubiquitous. To my mind, that undercuts the claim that changes of venue are warranted to ensure a fair trial. Domestic terrorist attacks are national stories. Obviously, people who live in a city that has been attacked stand a greater chance of knowing a victim of the attack, or of being personally affected by the fallout of the attack. But such jurors can easily be weeded out in a competent voir dire examination. Beyond that, nobody approves of terrorists. A terrorist who bombs Boston is not going to be viewed with detachment and objectivity if he is instead tried in Philadelphia or Houston. Moreover, the people in those cities are going to have been nearly as inundated by publicity about the atrocity as Bostonians.   In a criminal case, the issue with jurors is never whether they approve of egregious conduct. It is whether they can put aside their natural disapproval, figure out what factually happened, and faithfully apply the law as instructed by the judge. We’ll have to study the lengthy opinion. At first blush, though, it certainly appears that Friday’s ruling has at least as much to do with judicial hostility to capital punishment as to concerns about the due process implications of intense media coverage.\",\n",
    "\"2020-08-03, Tucker Carlson: Equality under law is slipping away The Jeffrey Epstein case paints a picture of a justice system in which the rich and well-connected can do virtually whatever they want. With America’s institutions under relentless attack — and in some cases crumbling — it’s worth thinking through what we’d like to save from the ashes. When the revolution finally ends. what do we hope to have left? In other words, what are our best traditions? There are a lot of them. At the very top of the list is equality under the law. Equality is the most basic of all American ideals. It’s the very first principle articulated in the Declaration of Independence. It’s why the founders broke with England. In America, all citizens would be subject to the same rules: The same standards. The same penalties. Rich or poor. Black or White. All of us are equal under the law. TUCKER CARLSON: POLITICAL VIOLENCE IS AN ATTACK ON AMERICA ITSELF That’s the promise. It’s easier to explain than to achieve, of course. But we’ve tried hard. We should be proud of that. Yet some in power are no longer trying. Equality, the thing we’ve fought to keep for centuries, is slipping away. The Jeffrey Epstein case is the latest example. Thursday night, dozens of unsealed court documents from the Epstein case emerged online. They paint a picture of a justice system in which the rich and well-connected can do virtually whatever they want. In one sworn deposition, a woman called Virginia Giuffre claims that Epstein and others — including Prince Andrew of England and attorney Alan Dershowitz — sexually abused her as a minor. Giuffre says an FBI agent responded that the agency didn’t plan to do anything about it. Epstein’s case, he allegedly said, wasn’t “going anywhere” because of “the chain of command.” The documents then describe what appears to be a remarkable abuse of power. Epstein’s accusers claim that Alan Dershowitz and Prince Andrew helped Epstein beat federal charges for sex crimes in 2008. They also allege that Dershowitz crafted the immunity agreement so that he himself also wouldn’t face criminal prosecution. Dershowitz has denied all of this.    Alex Acosta will not be joining us. He’s the former Labor Department secretary who at the time was a federal prosecutor. Acosta is the one who agreed to the Epstein deal. When asked why he let a sex abuser skate, Acosta reportedly said: “I was told Epstein ‘belonged to intelligence’ and to leave it alone.” What does that mean exactly? What intelligence service did Jeffrey Epstein work for? Why did our government allow him to sexually abuse little girls? We deserve answers to those questions. But so far, no one is providing them. Epstein can’t tell us. He’s dead. The press, strangely, doesn’t seem very interested in finding the answers. It’s possible that’s because Epstein was close to a remarkable number of prominent Democratic politicians.   The new documents suggest that former president Bill Clinton visited Epstein’s private island in the Caribbean with two young girls. Giuffre claims Epstein made her have sex with former New Mexico Gov. Bill Richardson, former U.S. Sen. George Mitchell, and a famous Massachusetts Institute of Technology scientist. Is any of this true? We don’t know. Ghislaine Maxwell probably does. She’s in custody and set to go on trial. Will she explain what happened and why? Doesn’t look promising. Her lawyers fought the release of these documents.\"]\n",
    "\n",
    "input_text = [input_arr[2]]\n",
    "#input_text = tfidf_transformer.transform(input_text)\n",
    "\n",
    "class_probabilities = text_clf.predict_proba(input_text)\n",
    "print(\"Left? %f Right? %f\" % (class_probabilities[0][0], class_probabilities[0][1]))\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Above: Testing against ALL sources gathered - most informative.\n",
    "# Below: Testing against respective datasets of the: 2, 4, or 6 sources trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_and_n_onegram_sr_nol_nopr.pkl     62.388\n",
      "h_and_n_onegram_sr_l_nopr.pkl       93.168\n",
      "h_and_n_bigram_sr_nol_nopr.pkl      94.634\n",
      "h_and_n_bigram_sr_l_nopr.pkl        93.038\n",
      "h_and_n_trigram_sr_nol_nopr.pkl     94.412\n",
      "h_and_n_trigram_sr_l_nopr.pkl       63.228\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl   92.742\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl     92.200\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl    93.189\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl      92.088\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl   93.100\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl     92.019\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl 91.606\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl   91.209\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl  91.671\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl    91.465\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl 92.011\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl   91.369\n",
      "h_and_n_onegram_sr_nol_pr.pkl       90.792\n",
      "h_and_n_onegram_sr_l_pr.pkl         89.197\n",
      "h_and_n_bigram_sr_nol_pr.pkl        90.465\n",
      "h_and_n_bigram_sr_l_pr.pkl          88.775\n",
      "h_and_n_trigram_sr_nol_pr.pkl       90.559\n",
      "h_and_n_trigram_sr_l_pr.pkl         89.127\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl     90.475\n",
      "hs_and_nr_onegram_sr_l_pr.pkl       89.376\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl      90.701\n",
      "hs_and_nr_bigram_sr_l_pr.pkl        89.369\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl     90.522\n",
      "hs_and_nr_trigram_sr_l_pr.pkl       89.157\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl   89.402\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl     88.193\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl    89.698\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl      63.148\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl   89.769\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl     57.719\n"
     ]
    }
   ],
   "source": [
    "# Determine the model to use with the highest accuracy relating to its specific dataset\n",
    "# So for h_and_n: This only compares the accuracy for testing between Huff and News\n",
    "# hs_and_nr: This compares the accuracy across testing against 4 sources: Huff, Salon, News, Redstate\n",
    "# hsr_and_nrw: Accuracy of testing against the 6 sources it trained on.\n",
    "\n",
    "#pickle_files.sort()\n",
    "final_results = []\n",
    "count = 0\n",
    "\n",
    "for x in range(len(pickle_files)):\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    if (count < 6 or (count >= 18 and count < 24)):\n",
    "        total_accuracy += (huff_acc[x] + newsmax_acc[x]) / 2\n",
    "        print(\"%-35s %.3f\" % (pickle_files[x], total_accuracy))\n",
    "        final_results.append([pickle_files[x], total_accuracy])\n",
    "    elif ((count >= 6 and count < 12) or (count >= 24 and count < 30)):\n",
    "        total_accuracy += (huff_acc[x] + newsmax_acc[x] + salon_acc[x] + redstate_acc[x]) / 4\n",
    "        print(\"%-35s %.3f\" % (pickle_files[x], total_accuracy))\n",
    "        final_results.append([pickle_files[x], total_accuracy])\n",
    "    else:\n",
    "        total_accuracy += (huff_acc[x] + newsmax_acc[x] + salon_acc[x] + redstate_acc[x] + rawstory_acc[x] + washington_acc[x]) / 6\n",
    "        print(\"%-35s %.3f\" % (pickle_files[x], total_accuracy))\n",
    "        final_results.append([pickle_files[x], total_accuracy])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine what .pkl file to load in each of the 6 different notebooks to illustrate best possible confusion matrix for that respective dataset of 2, 4, or 6 sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted final results:\n",
      "h_and_n_bigram_sr_nol_nopr.pkl      94.634\n",
      "h_and_n_trigram_sr_nol_nopr.pkl     94.412\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl    93.189\n",
      "h_and_n_onegram_sr_l_nopr.pkl       93.168\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl   93.100\n",
      "h_and_n_bigram_sr_l_nopr.pkl        93.038\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl   92.742\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl     92.200\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl      92.088\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl     92.019\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl 92.011\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl  91.671\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl 91.606\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl    91.465\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl   91.369\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl   91.209\n",
      "h_and_n_onegram_sr_nol_pr.pkl       90.792\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl      90.701\n",
      "h_and_n_trigram_sr_nol_pr.pkl       90.559\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl     90.522\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl     90.475\n",
      "h_and_n_bigram_sr_nol_pr.pkl        90.465\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl   89.769\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl    89.698\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl   89.402\n",
      "hs_and_nr_onegram_sr_l_pr.pkl       89.376\n",
      "hs_and_nr_bigram_sr_l_pr.pkl        89.369\n",
      "h_and_n_onegram_sr_l_pr.pkl         89.197\n",
      "hs_and_nr_trigram_sr_l_pr.pkl       89.157\n",
      "h_and_n_trigram_sr_l_pr.pkl         89.127\n",
      "h_and_n_bigram_sr_l_pr.pkl          88.775\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl     88.193\n",
      "h_and_n_trigram_sr_l_nopr.pkl       63.228\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl      63.148\n",
      "h_and_n_onegram_sr_nol_nopr.pkl     62.388\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl     57.719\n"
     ]
    }
   ],
   "source": [
    "# Use these results to determine which .pkl file to use and showcase the\n",
    "# confusion matrix for in each of the six training notebooks towards the end.\n",
    "print(\"Sorted final results:\")\n",
    "final_results.sort(key=lambda x: x[1], reverse=True)\n",
    "for x in range(len(final_results)):\n",
    "    print(\"%-35s %.3f\" % (final_results[x][0], final_results[x][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         onegram  bigram   trigram\n",
      "         93.168   94.634   94.412  \n",
      "         92.742   93.189   93.100  \n",
      "         92.200   93.038   92.019  \n",
      "         91.606   92.088   92.011  \n",
      "         91.209   91.671   91.369  \n",
      "         90.792   91.465   90.559  \n",
      "         90.475   90.701   90.522  \n",
      "         89.402   90.465   89.769  \n",
      "         89.376   89.698   89.157  \n",
      "         89.197   89.369   89.127  \n",
      "         88.193   88.775   63.228  \n",
      "         62.388   63.148   57.719  \n",
      "\n",
      "Overall: 88.396   89.020   86.083  \n"
     ]
    }
   ],
   "source": [
    "# Sort by n-gram\n",
    "pickle_results = {}\n",
    "onegram = []\n",
    "bigram = []\n",
    "trigram = []\n",
    "onegram_overall = 0\n",
    "bigram_overall = 0\n",
    "trigram_overall = 0\n",
    "total_one = 0\n",
    "total_bi = 0\n",
    "total_tri = 0\n",
    "\n",
    "# Separate the results according to gram usage.\n",
    "for x in range(len(final_results)):\n",
    "    if \"onegram\" in final_results[x][0]:\n",
    "        onegram.append(final_results[x][1])\n",
    "    elif \"bigram\" in final_results[x][0]:\n",
    "        bigram.append(final_results[x][1])\n",
    "    else:\n",
    "        trigram.append(final_results[x][1])\n",
    "\n",
    "# Dictionary\n",
    "#pickle_results[\"onegram\"] = onegram\n",
    "#pickle_results[\"bigram\"] = bigram\n",
    "#pickle_results[\"trigram\"] = trigram\n",
    "#for key, value in pickle_results.items():\n",
    "#    if key == \"onegram\":\n",
    "#        print(value)\n",
    "\n",
    "for x in range(len(onegram)):\n",
    "    total_one += onegram[x]\n",
    "    total_bi += bigram[x]\n",
    "    total_tri += trigram[x]\n",
    "onegram_overall = total_one / len(onegram)\n",
    "bigram_overall = total_bi / len(bigram)\n",
    "trigram_overall = total_tri / len(trigram)\n",
    "\n",
    "print(\"         onegram  bigram   trigram\")\n",
    "for x in range(len(onegram)):\n",
    "    print(\"         %.3f   %.3f   %.3f  \" % (onegram[x], bigram[x], trigram[x]))\n",
    "print(\"\\nOverall: %.3f   %.3f   %.3f  \" % (onegram_overall, bigram_overall, trigram_overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the difference between grams\n",
    "pickle_files_by_gram = ['h_and_n_onegram_sr_nol_nopr.pkl', 'h_and_n_bigram_sr_nol_nopr.pkl', 'h_and_n_trigram_sr_nol_nopr.pkl',\n",
    "                        'h_and_n_onegram_sr_l_nopr.pkl', 'h_and_n_bigram_sr_l_nopr.pkl', 'h_and_n_trigram_sr_l_nopr.pkl',\n",
    "                        'hs_and_nr_onegram_sr_nol_nopr.pkl', 'hs_and_nr_bigram_sr_nol_nopr.pkl', 'hs_and_nr_trigram_sr_nol_nopr.pkl', \n",
    "                        'hs_and_nr_onegram_sr_l_nopr.pkl', 'hs_and_nr_bigram_sr_l_nopr.pkl', 'hs_and_nr_trigram_sr_l_nopr.pkl',\n",
    "                        'hsr_and_nrw_onegram_sr_nol_nopr.pkl', 'hsr_and_nrw_bigram_sr_nol_nopr.pkl', 'hsr_and_nrw_trigram_sr_nol_nopr.pkl', \n",
    "                        'hsr_and_nrw_onegram_sr_l_nopr.pkl', 'hsr_and_nrw_bigram_sr_l_nopr.pkl', 'hsr_and_nrw_trigram_sr_l_nopr.pkl',\n",
    "                        'h_and_n_onegram_sr_nol_pr.pkl', 'h_and_n_bigram_sr_nol_pr.pkl', 'h_and_n_trigram_sr_nol_pr.pkl', \n",
    "                        'h_and_n_onegram_sr_l_pr.pkl', 'h_and_n_bigram_sr_l_pr.pkl', 'h_and_n_trigram_sr_l_pr.pkl',\n",
    "                        'hs_and_nr_onegram_sr_nol_pr.pkl', 'hs_and_nr_bigram_sr_nol_pr.pkl', 'hs_and_nr_trigram_sr_nol_pr.pkl', \n",
    "                        'hs_and_nr_onegram_sr_l_pr.pkl', 'hs_and_nr_bigram_sr_l_pr.pkl', 'hs_and_nr_trigram_sr_l_pr.pkl',\n",
    "                        'hsr_and_nrw_onegram_sr_nol_pr.pkl', 'hsr_and_nrw_bigram_sr_nol_pr.pkl', 'hsr_and_nrw_trigram_sr_nol_pr.pkl', \n",
    "                        'hsr_and_nrw_onegram_sr_l_pr.pkl', 'hsr_and_nrw_bigram_sr_l_pr.pkl', 'hsr_and_nrw_trigram_sr_l_pr.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h_and_n_onegram_sr_nol_nopr.pkl       62.388\n",
      "h_and_n_bigram_sr_nol_nopr.pkl        94.634\n",
      "h_and_n_trigram_sr_nol_nopr.pkl       94.412\n",
      "\n",
      "h_and_n_onegram_sr_l_nopr.pkl         93.168\n",
      "h_and_n_bigram_sr_l_nopr.pkl          93.038\n",
      "h_and_n_trigram_sr_l_nopr.pkl         63.228\n",
      "\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl     92.742\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl      93.189\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl     93.100\n",
      "\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl       92.200\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl        92.088\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl       92.019\n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl   91.606\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl    91.671\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl   92.011\n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl     91.209\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl      91.465\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl     91.369\n",
      "\n",
      "h_and_n_onegram_sr_nol_pr.pkl         90.792\n",
      "h_and_n_bigram_sr_nol_pr.pkl          90.465\n",
      "h_and_n_trigram_sr_nol_pr.pkl         90.559\n",
      "\n",
      "h_and_n_onegram_sr_l_pr.pkl           89.197\n",
      "h_and_n_bigram_sr_l_pr.pkl            88.775\n",
      "h_and_n_trigram_sr_l_pr.pkl           89.127\n",
      "\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl       90.475\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl        90.701\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl       90.522\n",
      "\n",
      "hs_and_nr_onegram_sr_l_pr.pkl         89.376\n",
      "hs_and_nr_bigram_sr_l_pr.pkl          89.369\n",
      "hs_and_nr_trigram_sr_l_pr.pkl         89.157\n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl     89.402\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl      89.698\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl     89.769\n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl       88.193\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl        63.148\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl       57.719\n",
      "\n",
      "Onegram > Bigram and Trigram accuracy:   6 / 12 times\n",
      "Bigram > Onegram and Bigram accuracy:    4 / 12 times\n",
      "Trigram > Onegram and Trigram accuracy:  2 / 12 times\n"
     ]
    }
   ],
   "source": [
    "grams = []\n",
    "one = 0\n",
    "bi = 0\n",
    "tri = 0\n",
    "one_count = 0\n",
    "bi_count = 0 \n",
    "tri_count = 0\n",
    "multiples = [2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35]\n",
    "\n",
    "for param in pickle_files_by_gram:\n",
    "    for x in range(len(final_results)):\n",
    "        if param in final_results[x][0]:\n",
    "            grams.append((final_results[x][0], final_results[x][1]))\n",
    "\n",
    "for x in range(len(grams)):\n",
    "    # Mutiples of 3...\n",
    "    if (x % 3 == 0):\n",
    "        if x == 0:\n",
    "            one = grams[x][1]\n",
    "        if one == 0:\n",
    "            one = grams[x][1]\n",
    "        print(\"\\n%-35s   %.3f\" % (grams[x][0], grams[x][1]))\n",
    "    else:\n",
    "        if bi == 0:\n",
    "            bi = grams[x][1]\n",
    "        if x in multiples:\n",
    "            tri = grams[x][1]\n",
    "        print(\"%-35s   %.3f\" % (grams[x][0], grams[x][1]))\n",
    "    \n",
    "    # As x reaches a number in the list, clear variables and loop again\n",
    "    if x in multiples:\n",
    "        if (one > bi and one > tri):\n",
    "            one_count += 1\n",
    "        elif (bi > one and bi > tri):\n",
    "            bi_count += 1\n",
    "        else:\n",
    "            tri_count += 1\n",
    "        one = 0\n",
    "        bi = 0\n",
    "        tri = 0\n",
    "\n",
    "print(\"\\nOnegram > Bigram and Trigram accuracy:   %i / %i times\" % (one_count, len(pickle_files_by_gram) / 3))\n",
    "print(\"Bigram > Onegram and Bigram accuracy:    %i / %i times\" % (bi_count, len(pickle_files_by_gram) / 3))\n",
    "print(\"Trigram > Onegram and Trigram accuracy:  %i / %i times\" % (tri_count, len(pickle_files_by_gram) / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the difference between lemmatization vs no lemmatization\n",
    "pickle_files_by_lemmatization = ['h_and_n_onegram_sr_nol_nopr.pkl', 'h_and_n_onegram_sr_l_nopr.pkl',\n",
    "                'h_and_n_bigram_sr_nol_nopr.pkl', 'h_and_n_bigram_sr_l_nopr.pkl',\n",
    "                'h_and_n_trigram_sr_nol_nopr.pkl', 'h_and_n_trigram_sr_l_nopr.pkl',\n",
    "                'hs_and_nr_onegram_sr_nol_nopr.pkl', 'hs_and_nr_onegram_sr_l_nopr.pkl',\n",
    "                'hs_and_nr_bigram_sr_nol_nopr.pkl', 'hs_and_nr_bigram_sr_l_nopr.pkl',\n",
    "                'hs_and_nr_trigram_sr_nol_nopr.pkl', 'hs_and_nr_trigram_sr_l_nopr.pkl',\n",
    "                'hsr_and_nrw_onegram_sr_nol_nopr.pkl', 'hsr_and_nrw_onegram_sr_l_nopr.pkl',\n",
    "                'hsr_and_nrw_bigram_sr_nol_nopr.pkl', 'hsr_and_nrw_bigram_sr_l_nopr.pkl',\n",
    "                'hsr_and_nrw_trigram_sr_nol_nopr.pkl', 'hsr_and_nrw_trigram_sr_l_nopr.pkl',\n",
    "                'h_and_n_onegram_sr_nol_pr.pkl', 'h_and_n_onegram_sr_l_pr.pkl',\n",
    "                'h_and_n_bigram_sr_nol_pr.pkl', 'h_and_n_bigram_sr_l_pr.pkl',\n",
    "                'h_and_n_trigram_sr_nol_pr.pkl', 'h_and_n_trigram_sr_l_pr.pkl',\n",
    "                'hs_and_nr_onegram_sr_nol_pr.pkl', 'hs_and_nr_onegram_sr_l_pr.pkl',\n",
    "                'hs_and_nr_bigram_sr_nol_pr.pkl', 'hs_and_nr_bigram_sr_l_pr.pkl',\n",
    "                'hs_and_nr_trigram_sr_nol_pr.pkl', 'hs_and_nr_trigram_sr_l_pr.pkl',\n",
    "                'hsr_and_nrw_onegram_sr_nol_pr.pkl', 'hsr_and_nrw_onegram_sr_l_pr.pkl',\n",
    "                'hsr_and_nrw_bigram_sr_nol_pr.pkl', 'hsr_and_nrw_bigram_sr_l_pr.pkl',\n",
    "                'hsr_and_nrw_trigram_sr_nol_pr.pkl', 'hsr_and_nrw_trigram_sr_l_pr.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h_and_n_onegram_sr_nol_nopr.pkl       62.388\n",
      "h_and_n_onegram_sr_l_nopr.pkl         93.168\n",
      "\n",
      "h_and_n_bigram_sr_nol_nopr.pkl        94.634\n",
      "h_and_n_bigram_sr_l_nopr.pkl          93.038\n",
      "\n",
      "h_and_n_trigram_sr_nol_nopr.pkl       94.412\n",
      "h_and_n_trigram_sr_l_nopr.pkl         63.228\n",
      "\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl     92.742\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl       92.200\n",
      "\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl      93.189\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl        92.088\n",
      "\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl     93.100\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl       92.019\n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl   91.606\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl     91.209\n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl    91.671\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl      91.465\n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl   92.011\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl     91.369\n",
      "\n",
      "h_and_n_onegram_sr_nol_pr.pkl         90.792\n",
      "h_and_n_onegram_sr_l_pr.pkl           89.197\n",
      "\n",
      "h_and_n_bigram_sr_nol_pr.pkl          90.465\n",
      "h_and_n_bigram_sr_l_pr.pkl            88.775\n",
      "\n",
      "h_and_n_trigram_sr_nol_pr.pkl         90.559\n",
      "h_and_n_trigram_sr_l_pr.pkl           89.127\n",
      "\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl       90.475\n",
      "hs_and_nr_onegram_sr_l_pr.pkl         89.376\n",
      "\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl        90.701\n",
      "hs_and_nr_bigram_sr_l_pr.pkl          89.369\n",
      "\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl       90.522\n",
      "hs_and_nr_trigram_sr_l_pr.pkl         89.157\n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl     89.402\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl       88.193\n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl      89.698\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl        63.148\n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl     89.769\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl       57.719\n",
      "\n",
      "No Lemmatization > Lemmatization accuracy:   17 / 18 times\n",
      "Lemmatization > No Lemmatization accuracy:   1 / 18 times\n"
     ]
    }
   ],
   "source": [
    "lemmas = []\n",
    "one = 0\n",
    "bi = 0\n",
    "one_count = 0\n",
    "bi_count = 0 \n",
    "multiples = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35]\n",
    "\n",
    "for param in pickle_files_by_lemmatization:\n",
    "    for x in range(len(final_results)):\n",
    "        if param in final_results[x][0]:\n",
    "            lemmas.append((final_results[x][0], final_results[x][1]))\n",
    "\n",
    "for x in range(len(lemmas)):\n",
    "    # Mutiples of 2...\n",
    "    if (x % 2 == 0):\n",
    "        if x == 0:\n",
    "            one = lemmas[x][1]\n",
    "        if one == 0:\n",
    "            one = lemmas[x][1]\n",
    "        print(\"\\n%-35s   %.3f\" % (lemmas[x][0], lemmas[x][1]))\n",
    "    else:\n",
    "        if bi == 0:\n",
    "            bi = lemmas[x][1]\n",
    "        print(\"%-35s   %.3f\" % (lemmas[x][0], lemmas[x][1]))\n",
    "        \n",
    "    # As x reaches a number in the list, clear variables and loop again\n",
    "    if x in multiples:\n",
    "        if (one > bi):\n",
    "            one_count += 1\n",
    "        else:\n",
    "            bi_count += 1\n",
    "        one = 0\n",
    "        bi = 0\n",
    "\n",
    "print(\"\\nNo Lemmatization > Lemmatization accuracy:   %i / %i times\" % (one_count, len(pickle_files_by_lemmatization) / 2))\n",
    "print(\"Lemmatization > No Lemmatization accuracy:   %i / %i times\" % (bi_count, len(pickle_files_by_lemmatization) / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the difference between lemmatization vs no lemmatization\n",
    "pickle_files_by_propernoun = ['h_and_n_onegram_sr_nol_nopr.pkl', 'h_and_n_onegram_sr_nol_pr.pkl',\n",
    "                              'h_and_n_onegram_sr_l_nopr.pkl', 'h_and_n_onegram_sr_l_pr.pkl',\n",
    "                              'h_and_n_bigram_sr_nol_nopr.pkl', 'h_and_n_bigram_sr_nol_pr.pkl',\n",
    "                              'h_and_n_bigram_sr_l_nopr.pkl', 'h_and_n_bigram_sr_l_pr.pkl',\n",
    "                              'h_and_n_trigram_sr_nol_nopr.pkl', 'h_and_n_trigram_sr_nol_pr.pkl',\n",
    "                              'h_and_n_trigram_sr_l_nopr.pkl', 'h_and_n_trigram_sr_l_pr.pkl',\n",
    "                              'hs_and_nr_onegram_sr_nol_nopr.pkl', 'hs_and_nr_onegram_sr_nol_pr.pkl',\n",
    "                              'hs_and_nr_onegram_sr_l_nopr.pkl', 'hs_and_nr_onegram_sr_l_pr.pkl',\n",
    "                              'hs_and_nr_bigram_sr_nol_nopr.pkl', 'hs_and_nr_bigram_sr_nol_pr.pkl',\n",
    "                              'hs_and_nr_bigram_sr_l_nopr.pkl', 'hs_and_nr_bigram_sr_l_pr.pkl',\n",
    "                              'hs_and_nr_trigram_sr_nol_nopr.pkl', 'hs_and_nr_trigram_sr_nol_pr.pkl',\n",
    "                              'hs_and_nr_trigram_sr_l_nopr.pkl', 'hs_and_nr_trigram_sr_l_pr.pkl',\n",
    "                              'hsr_and_nrw_onegram_sr_nol_nopr.pkl', 'hsr_and_nrw_onegram_sr_nol_pr.pkl',\n",
    "                              'hsr_and_nrw_onegram_sr_l_nopr.pkl', 'hsr_and_nrw_onegram_sr_l_pr.pkl',\n",
    "                              'hsr_and_nrw_bigram_sr_nol_nopr.pkl', 'hsr_and_nrw_bigram_sr_nol_pr.pkl',\n",
    "                              'hsr_and_nrw_bigram_sr_l_nopr.pkl', 'hsr_and_nrw_bigram_sr_l_pr.pkl',\n",
    "                              'hsr_and_nrw_trigram_sr_nol_nopr.pkl', 'hsr_and_nrw_trigram_sr_nol_pr.pkl',\n",
    "                              'hsr_and_nrw_trigram_sr_l_nopr.pkl', 'hsr_and_nrw_trigram_sr_l_pr.pkl']         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "h_and_n_onegram_sr_nol_nopr.pkl       62.388\n",
      "h_and_n_onegram_sr_nol_pr.pkl         90.792\n",
      "\n",
      "h_and_n_onegram_sr_l_nopr.pkl         93.168\n",
      "h_and_n_onegram_sr_l_pr.pkl           89.197\n",
      "\n",
      "h_and_n_bigram_sr_nol_nopr.pkl        94.634\n",
      "h_and_n_bigram_sr_nol_pr.pkl          90.465\n",
      "\n",
      "h_and_n_bigram_sr_l_nopr.pkl          93.038\n",
      "h_and_n_bigram_sr_l_pr.pkl            88.775\n",
      "\n",
      "h_and_n_trigram_sr_nol_nopr.pkl       94.412\n",
      "h_and_n_trigram_sr_nol_pr.pkl         90.559\n",
      "\n",
      "h_and_n_trigram_sr_l_nopr.pkl         63.228\n",
      "h_and_n_trigram_sr_l_pr.pkl           89.127\n",
      "\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl     92.742\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl       90.475\n",
      "\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl       92.200\n",
      "hs_and_nr_onegram_sr_l_pr.pkl         89.376\n",
      "\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl      93.189\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl        90.701\n",
      "\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl        92.088\n",
      "hs_and_nr_bigram_sr_l_pr.pkl          89.369\n",
      "\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl     93.100\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl       90.522\n",
      "\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl       92.019\n",
      "hs_and_nr_trigram_sr_l_pr.pkl         89.157\n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl   91.606\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl     89.402\n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl     91.209\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl       88.193\n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl    91.671\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl      89.698\n",
      "\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl      91.465\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl        63.148\n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl   92.011\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl     89.769\n",
      "\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl     91.369\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl       57.719\n",
      "\n",
      "No Proper Noun Removal > Proper Noun Removal accuracy:   16 / 18 times\n",
      "Proper Noun Removal > No Proper Noun Removal accuracy:   2 / 18 times\n"
     ]
    }
   ],
   "source": [
    "propernoun = []\n",
    "one = 0\n",
    "bi = 0\n",
    "one_count = 0\n",
    "bi_count = 0 \n",
    "multiples = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35]\n",
    "\n",
    "for param in pickle_files_by_propernoun:\n",
    "    for x in range(len(final_results)):\n",
    "        if param in final_results[x][0]:\n",
    "            propernoun.append((final_results[x][0], final_results[x][1]))\n",
    "\n",
    "for x in range(len(propernoun)):\n",
    "    # Mutiples of 2...\n",
    "    if (x % 2 == 0):\n",
    "        if x == 0:\n",
    "            one = propernoun[x][1]\n",
    "        if one == 0:\n",
    "            one = propernoun[x][1]\n",
    "        print(\"\\n%-35s   %.3f\" % (propernoun[x][0], propernoun[x][1]))\n",
    "    else:\n",
    "        if bi == 0:\n",
    "            bi = propernoun[x][1]\n",
    "        print(\"%-35s   %.3f\" % (propernoun[x][0], propernoun[x][1]))\n",
    "        \n",
    "    # As x reaches a number in the list, clear variables and loop again\n",
    "    if x in multiples:\n",
    "        if (one > bi):\n",
    "            one_count += 1\n",
    "        else:\n",
    "            bi_count += 1\n",
    "        one = 0\n",
    "        bi = 0\n",
    "\n",
    "print(\"\\nNo Proper Noun Removal > Proper Noun Removal accuracy:   %i / %i times\" % (one_count, len(pickle_files_by_propernoun) / 2))\n",
    "print(\"Proper Noun Removal > No Proper Noun Removal accuracy:   %i / %i times\" % (bi_count, len(pickle_files_by_propernoun) / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_and_n_onegram_sr_nol_nopr.pkl\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_onegram_sr_nol_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_onegram_sr_l_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_onegram_sr_l_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_nol_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_nol_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_l_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_l_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_nol_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_nol_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_l_nopr.pkl\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_l_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hs_and_nr_onegram_sr_l_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_l_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_l_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsr_and_nrw_bigram_sr_nol_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_bigram_sr_l_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access the classifier and CountVectorizer() in the Pipeline object from the .pkl file\n",
    "\n",
    "for pickle in pickle_files_by_propernoun:\n",
    "    text_clf = joblib.load(pickle)\n",
    "    \n",
    "    try:\n",
    "        clf = text_clf.named_steps['clf']\n",
    "        count_vect = text_clf.named_steps['vect']\n",
    "        print(pickle)\n",
    "        print(clf)\n",
    "        print(\"%s \\n\" % (count_vect))\n",
    "    except:\n",
    "        clf = text_clf.named_steps['eclf']\n",
    "        count_vect = text_clf.named_steps['vect']\n",
    "        print(pickle)\n",
    "        print(clf)\n",
    "        print(\"%s \\n\" % (count_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_and_n_bigram_sr_nol_nopr.pkl: 94.634\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_nol_nopr.pkl: 94.412\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_nol_nopr.pkl: 93.189\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_onegram_sr_l_nopr.pkl: 93.168\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_nol_nopr.pkl: 93.100\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_l_nopr.pkl: 93.038\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_nol_nopr.pkl: 92.742\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_l_nopr.pkl: 92.200\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_l_nopr.pkl: 92.088\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_l_nopr.pkl: 92.019\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_nopr.pkl: 92.011\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_nopr.pkl: 91.671\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_nopr.pkl: 91.606\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsr_and_nrw_bigram_sr_l_nopr.pkl: 91.465\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_l_nopr.pkl: 91.369\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_nopr.pkl: 91.209\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_onegram_sr_nol_pr.pkl: 90.792\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_nol_pr.pkl: 90.701\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_nol_pr.pkl: 90.559\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_nol_pr.pkl: 90.522\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_nol_pr.pkl: 90.475\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_nol_pr.pkl: 90.465\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_nol_pr.pkl: 89.769\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_bigram_sr_nol_pr.pkl: 89.698\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_nol_pr.pkl: 89.402\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_onegram_sr_l_pr.pkl: 89.376\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_bigram_sr_l_pr.pkl: 89.369\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_and_n_onegram_sr_l_pr.pkl: 89.197\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hs_and_nr_trigram_sr_l_pr.pkl: 89.157\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_l_pr.pkl: 89.127\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_bigram_sr_l_pr.pkl: 88.775\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_onegram_sr_l_pr.pkl: 88.193\n",
      "VotingClassifier(estimators=[('svc',\n",
      "                              CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0))),\n",
      "                             ('sgd',\n",
      "                              CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=0))),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=0))],\n",
      "                 voting='soft')\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "h_and_n_trigram_sr_l_nopr.pkl: 63.228\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n",
      "hsr_and_nrw_bigram_sr_l_pr.pkl: 63.148\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 2),\n",
      "                stop_words='english') \n",
      "\n",
      "h_and_n_onegram_sr_nol_nopr.pkl: 62.388\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, stop_words='english') \n",
      "\n",
      "hsr_and_nrw_trigram_sr_l_pr.pkl: 57.719\n",
      "RandomForestClassifier(n_estimators=200, random_state=0)\n",
      "CountVectorizer(max_df=0.75, max_features=5000, min_df=4, ngram_range=(1, 3),\n",
      "                stop_words='english') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(final_results)):\n",
    "    text_clf = joblib.load(final_results[x][0])\n",
    "    \n",
    "    try:\n",
    "        clf = text_clf.named_steps['clf']\n",
    "        count_vect = text_clf.named_steps['vect']\n",
    "        print(\"%s: %.3f\" % (final_results[x][0], final_results[x][1]))\n",
    "        print(clf)\n",
    "        print(\"%s \\n\" % (count_vect))\n",
    "    except:\n",
    "        clf = text_clf.named_steps['eclf']\n",
    "        count_vect = text_clf.named_steps['vect']\n",
    "        print(\"%s: %.3f\" % (final_results[x][0], final_results[x][1]))\n",
    "        print(clf)\n",
    "        print(\"%s \\n\" % (count_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows you the most frequently occurring words seen in text labelled as left or right\n",
    "# This works on the most recent clf, but should relatively represent the same features used by each classifier\n",
    "# so long as the parameters for min_df and max_df for each classifier are the same\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    top_features = []\n",
    "    feature_names = vectorizer.get_feature_names() # Get all the feature names that CountVectorizer() is using\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names)) # Put all feature names with their weights. Sort.\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1]) \n",
    "    print(\"\\t\\t%-15s\\t\\t\\t%-15s\" % ('Left', 'Right'))\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))    \n",
    "        top_features.append([fn_1, coef_1, fn_2, coef_2])\n",
    "    print()\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VotingClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-306bcd7cd8bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eclf'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vect'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtop_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshow_most_informative_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_vect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-99d7750c3529>\u001b[0m in \u001b[0;36mshow_most_informative_features\u001b[1;34m(vectorizer, clf, n)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtop_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Get all the feature names that CountVectorizer() is using\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mcoefs_with_fns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Put all feature names with their weights. Sort.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoefs_with_fns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoefs_with_fns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\\t%-15s\\t\\t\\t%-15s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VotingClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "# RandomForest and VotingClassifier - Have no coef_ function\n",
    "# Therefore, you cannot see the frequency of each feature those classifiers are using\n",
    "# You'll need to use the 2nd best classifier to see the frequency of the top features.\n",
    "# Use LinearSVC classifier in Top_Features.ipynb (SGD - 3rd best)\n",
    "text_clf = joblib.load(final_results[1][0])\n",
    "clf = text_clf.named_steps['eclf']\n",
    "count_vect = text_clf.named_steps['vect']\n",
    "top_features = show_most_informative_features(count_vect, clf, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
